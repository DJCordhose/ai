{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "berater-v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/berater-v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eU7ylMh1kQ2y"
      },
      "cell_type": "markdown",
      "source": [
        "# Berater Environment v3\n",
        "\n",
        "## Changes from v2\n",
        "1. changed learning strategy from ppo to dqn (seems more intuitive)\n",
        "1. Fixed bug in env#reset still returning the the scalar position instead of the complete state  \n",
        "\n",
        "## Next Steps\n",
        "1. Make monitor files work and plot performance\n",
        "1. choose costs of traversal randomly with each episode\n",
        "  * aim: agent will (hopefully) be able to work with any costs\n",
        "1. train a different graph with each episode \n",
        "  * aim: agent can work on any graph"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zpzHtN3-kQ26"
      },
      "cell_type": "markdown",
      "source": [
        "## Installation (required for colab)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0E567zPTkQ28",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/openai/baselines >/dev/null\n",
        "# !pip install gym >/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7Y3XKzw1kQ3I",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnt=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-S4sZG5ZkQ3T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import gym\n",
        "from gym.utils import seeding\n",
        "from gym import spaces\n",
        "\n",
        "def state_name_to_int(state):\n",
        "    state_name_map = {\n",
        "        'S': 0,\n",
        "        'A': 1,\n",
        "        'B': 2,\n",
        "        'C': 3,\n",
        "    }\n",
        "    return state_name_map[state]\n",
        "\n",
        "def int_to_state_name(state_as_int):\n",
        "    state_map = {\n",
        "        0: 'S',\n",
        "        1: 'A',\n",
        "        2: 'B',\n",
        "        3: 'C'\n",
        "    }\n",
        "    return state_map[state_as_int]\n",
        "    \n",
        "class BeraterEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    The Berater Problem\n",
        "\n",
        "    Actions: \n",
        "    There are 3 discrete deterministic actions:\n",
        "    - 0: First Direction\n",
        "    - 1: Second Direction\n",
        "    - 2: Third Direction / Go home\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['ansi']}\n",
        "    \n",
        "    num_envs = 1\n",
        "    showStep = False\n",
        "    showDone = True\n",
        "    showRender = False\n",
        "    envEpisodeModulo = 100\n",
        "\n",
        "    def __init__(self):\n",
        "        self.map = {\n",
        "            'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
        "            'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
        "            'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
        "            'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
        "        }\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(low=numpy.array([0,-1000,-1000,-1000,-1000,-1000,-1000]),\n",
        "                                             high=numpy.array([3,1000,1000,1000,1000,1000,1000]),\n",
        "                                             dtype=numpy.float32)\n",
        "\n",
        "\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.envReward = 0\n",
        "        self.envEpisodeCount = 0\n",
        "        self.envStepCount = 0\n",
        "\n",
        "        self.reset()\n",
        "        self.optimum = self.calculate_customers_reward()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, actionArg):\n",
        "        paths = self.map[self.state]\n",
        "        action = actionArg\n",
        "        destination, cost = paths[action]\n",
        "        lastState = self.state\n",
        "        lastObState = state_name_to_int(lastState)\n",
        "        customerReward = self.customer_reward[destination]\n",
        "\n",
        "        info = {\"from\": self.state, \"to\": destination}\n",
        "\n",
        "        self.state = destination\n",
        "        reward = (-cost + self.customer_reward[destination]) / self.optimum\n",
        "        self.customer_visited(destination)\n",
        "        done = destination == 'S' and self.all_customers_visited()\n",
        "\n",
        "        stateAsInt = state_name_to_int(self.state)\n",
        "        self.totalReward += reward\n",
        "        self.stepCount += 1\n",
        "        self.envReward += reward\n",
        "        self.envStepCount += 1\n",
        "\n",
        "        if self.showStep:\n",
        "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
        "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
        "                   #lastState + ':' + str(lastObState) + ' --' + str(action) + '-> ' + self.state + ':' + str(stateAsInt) +\n",
        "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
        "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
        "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
        "                   )\n",
        "\n",
        "        if done and not self.isDone:\n",
        "            self.envEpisodeCount += 1\n",
        "            if BeraterEnv.showDone or (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                episodes = BeraterEnv.envEpisodeModulo\n",
        "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
        "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
        "                print( \"Done: \" + \n",
        "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
        "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
        "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
        "                        )\n",
        "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                    self.envReward = 0\n",
        "                    self.envStepCount = 0\n",
        "\n",
        "        self.isDone = done\n",
        "        observation = self.getObservation(stateAsInt)\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def getObservation(self, position):\n",
        "        result = numpy.array([ position, \n",
        "                               self.getEdgeObservation('S','A'),\n",
        "                               self.getEdgeObservation('S','B'),\n",
        "                               self.getEdgeObservation('S','C'),\n",
        "                               self.getEdgeObservation('A','B'),\n",
        "                               self.getEdgeObservation('A','C'),\n",
        "                               self.getEdgeObservation('B','C'),\n",
        "                              ],\n",
        "                             dtype=numpy.float32)\n",
        "        return result\n",
        "\n",
        "    def getEdgeObservation(self, source, target):\n",
        "        reward = self.customer_reward[target] \n",
        "        cost = self.getCost(source,target)\n",
        "        result = reward - cost\n",
        "\n",
        "        return result\n",
        "\n",
        "    def getCost(self, source, target):\n",
        "        paths = self.map[source]\n",
        "        targetIndex=state_name_to_int(target)\n",
        "        for destination, cost in paths:\n",
        "            if destination == target:\n",
        "                result = cost\n",
        "                break\n",
        "\n",
        "        return result\n",
        "\n",
        "    def customer_visited(self, customer):\n",
        "        self.customer_reward[customer] = 0\n",
        "\n",
        "    def all_customers_visited(self):\n",
        "        return self.calculate_customers_reward() == 0\n",
        "\n",
        "    def calculate_customers_reward(self):\n",
        "        sum = 0\n",
        "        for value in self.customer_reward.values():\n",
        "            sum += value\n",
        "        return sum\n",
        "\n",
        "    def reset(self):\n",
        "        # print(\"Reset\")\n",
        "        \n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "        reward_per_customer = 1000\n",
        "        self.customer_reward = {\n",
        "            'S': 0,\n",
        "            'A': reward_per_customer,\n",
        "            'B': reward_per_customer,\n",
        "            'C': reward_per_customer,\n",
        "        }\n",
        "\n",
        "        self.state = 'S'\n",
        "        return self.getObservation(state_name_to_int(self.state))\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        if BeraterEnv.showRender:\n",
        "            print( (\"steps=%4.0f  \" % self.stepCount) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + ' done=' + str(self.isDone))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kWF5vSsakQ3b"
      },
      "cell_type": "markdown",
      "source": [
        "# Register Einvornment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SU1NxPMokQ3e",
        "outputId": "111fd782-dd78-48fe-bcf5-971320852bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from gym.envs.registration import register\n",
        "\n",
        "cnt += 1\n",
        "id = \"Berater-v{}\".format(cnt)\n",
        "register(\n",
        "    id=id,\n",
        "    entry_point=BeraterEnv\n",
        ")   \n",
        "\n",
        "print(id)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Berater-v1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Usj9iWTskQ3t"
      },
      "cell_type": "markdown",
      "source": [
        "# Try out Environment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oTtUfeONkQ3w",
        "outputId": "5697e14e-d6a0-412e-c69c-6a21ef021300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = True\n",
        "BeraterEnv.showDone = True\n",
        "\n",
        "env = gym.make(id)\n",
        "observation = env.reset()\n",
        "print(env)\n",
        "\n",
        "for t in range(1000):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        env.render()\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BeraterEnv<Berater-v1>>\n",
            "Episode:    0   Step:    1  S --0-> A R= 0.30 totalR= 0.30 cost= 100 customerR=1000 optimum=3000\n",
            "Episode:    0   Step:    2  A --1-> C R= 0.20 totalR= 0.50 cost= 400 customerR=1000 optimum=3000\n",
            "Episode:    0   Step:    3  C --0-> A R=-0.13 totalR= 0.37 cost= 400 customerR=   0 optimum=3000\n",
            "Episode:    0   Step:    4  A --1-> C R=-0.13 totalR= 0.23 cost= 400 customerR=   0 optimum=3000\n",
            "Episode:    0   Step:    5  C --1-> B R= 0.25 totalR= 0.48 cost= 250 customerR=1000 optimum=3000\n",
            "Episode:    0   Step:    6  B --2-> S R=-0.13 totalR= 0.35 cost= 400 customerR=   0 optimum=3000\n",
            "Done: episodes=     1  avgSteps=  6.00  avgTotalReward= 0.35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4GlYjZ3xkQ38"
      },
      "cell_type": "markdown",
      "source": [
        "# Train model\n",
        "\n",
        "* 0.73 would be perfect total reward"
      ]
    },
    {
      "metadata": {
        "id": "5yXW73oNRfPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "217803b0-fce3-4306-c3bc-2d460e1e97c0"
      },
      "cell_type": "code",
      "source": [
        "%env OPENAI_LOGDIR=/content/logs/berater\n",
        "# %env OPENAI_LOG_FORMAT=csv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: OPENAI_LOGDIR=/content/logs/berater\n",
            "env: OPENAI_LOG_FORMAT=csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VbSZSjeARhfk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "2b4777fb-1d81-4f03-aaf3-99db51d80a0a"
      },
      "cell_type": "code",
      "source": [
        "%env"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CLICOLOR': '1',\n",
              " 'CLOUDSDK_CONFIG': '/content/.config',\n",
              " 'COLAB_GPU': '1',\n",
              " 'CUDA_PKG_VERSION': '9-2=9.2.148-1',\n",
              " 'CUDA_VERSION': '9.2.148',\n",
              " 'CUDNN_VERSION': '7.4.1.5',\n",
              " 'DATALAB_SETTINGS_OVERRIDES': '{\"kernelManagerProxyPort\":6000,\"kernelManagerProxyHost\":\"172.28.0.3\",\"jupyterArgs\":[\"--ip=\\\\\"172.28.0.2\\\\\"\"]}',\n",
              " 'DEBIAN_FRONTEND': 'noninteractive',\n",
              " 'ENV': '/root/.bashrc',\n",
              " 'GIT_PAGER': 'cat',\n",
              " 'GLIBCPP_FORCE_NEW': '1',\n",
              " 'GLIBCXX_FORCE_NEW': '1',\n",
              " 'HOME': '/root',\n",
              " 'HOSTNAME': '4664b721f1d5',\n",
              " 'JPY_PARENT_PID': '68',\n",
              " 'LANG': 'en_US.UTF-8',\n",
              " 'LD_LIBRARY_PATH': '/usr/lib64-nvidia',\n",
              " 'LD_PRELOAD': '/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4',\n",
              " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
              " 'NCCL_VERSION': '2.3.7',\n",
              " 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility',\n",
              " 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.2',\n",
              " 'NVIDIA_VISIBLE_DEVICES': 'all',\n",
              " 'OLDPWD': '/',\n",
              " 'OPENAI_LOGDIR': '/content/logs/berater',\n",
              " 'OPENAI_LOG_FORMAT': 'csv',\n",
              " 'PAGER': 'cat',\n",
              " 'PATH': '/usr/local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin',\n",
              " 'PWD': '/',\n",
              " 'PYTHONPATH': '/env/python',\n",
              " 'SHELL': '/bin/bash',\n",
              " 'SHLVL': '1',\n",
              " 'TERM': 'xterm-color',\n",
              " 'TF_FORCE_GPU_ALLOW_GROWTH': 'true',\n",
              " '_': '/tools/node/bin/forever',\n",
              " '__EGL_VENDOR_LIBRARY_DIRS': '/usr/lib64-nvidia:/usr/share/glvnd/egl_vendor.d/'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NzbylmYAkQ3-",
        "outputId": "191f30ff-0827-420c-fe21-8998db8a7fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1207
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from baselines import deepq\n",
        "\n",
        "# \n",
        "\n",
        "BeraterEnv.showStep = False\n",
        "BeraterEnv.showDone = False\n",
        "\n",
        "env = gym.make(id)\n",
        "\n",
        "# https://en.wikipedia.org/wiki/Q-learning#Influence_of_variables\n",
        "%time model = deepq.learn(\\\n",
        "        env,\\\n",
        "        seed=42,\\\n",
        "        network='mlp',\\\n",
        "        lr=1e-3,\\\n",
        "        total_timesteps=30000,\\\n",
        "        buffer_size=50000,\\\n",
        "        exploration_fraction=0.5,\\\n",
        "        exploration_final_eps=0.02,\\\n",
        "        print_freq=1000)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done: episodes=   100  avgSteps=  8.88  avgTotalReward= 0.21\n",
            "Done: episodes=   200  avgSteps=  8.21  avgTotalReward= 0.27\n",
            "Done: episodes=   300  avgSteps=  7.92  avgTotalReward= 0.30\n",
            "Done: episodes=   400  avgSteps=  7.50  avgTotalReward= 0.36\n",
            "Done: episodes=   500  avgSteps=  6.92  avgTotalReward= 0.39\n",
            "Done: episodes=   600  avgSteps=  6.53  avgTotalReward= 0.43\n",
            "Done: episodes=   700  avgSteps=  6.28  avgTotalReward= 0.45\n",
            "Done: episodes=   800  avgSteps=  6.36  avgTotalReward= 0.46\n",
            "Done: episodes=   900  avgSteps=  6.02  avgTotalReward= 0.48\n",
            "Done: episodes=  1000  avgSteps=  5.73  avgTotalReward= 0.50\n",
            "Done: episodes=  1100  avgSteps=  5.69  avgTotalReward= 0.52\n",
            "Done: episodes=  1200  avgSteps=  5.30  avgTotalReward= 0.55\n",
            "Done: episodes=  1300  avgSteps=  5.15  avgTotalReward= 0.57\n",
            "Done: episodes=  1400  avgSteps=  5.03  avgTotalReward= 0.58\n",
            "Done: episodes=  1500  avgSteps=  4.93  avgTotalReward= 0.60\n",
            "Done: episodes=  1600  avgSteps=  4.87  avgTotalReward= 0.60\n",
            "Done: episodes=  1700  avgSteps=  4.94  avgTotalReward= 0.60\n",
            "Done: episodes=  1800  avgSteps=  4.62  avgTotalReward= 0.65\n",
            "Done: episodes=  1900  avgSteps=  4.81  avgTotalReward= 0.62\n",
            "Done: episodes=  2000  avgSteps=  4.58  avgTotalReward= 0.65\n",
            "Done: episodes=  2100  avgSteps=  4.58  avgTotalReward= 0.65\n",
            "Done: episodes=  2200  avgSteps=  4.52  avgTotalReward= 0.66\n",
            "Done: episodes=  2300  avgSteps=  4.36  avgTotalReward= 0.69\n",
            "Done: episodes=  2400  avgSteps=  4.17  avgTotalReward= 0.70\n",
            "Done: episodes=  2500  avgSteps=  4.13  avgTotalReward= 0.71\n",
            "Done: episodes=  2600  avgSteps=  4.17  avgTotalReward= 0.71\n",
            "Done: episodes=  2700  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  2800  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes=  2900  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  3000  avgSteps=  4.06  avgTotalReward= 0.73\n",
            "Done: episodes=  3100  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  3200  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes=  3300  avgSteps=  4.10  avgTotalReward= 0.72\n",
            "Done: episodes=  3400  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes=  3500  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  3600  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  3700  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  3800  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  3900  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  4000  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes=  4100  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  4200  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  4300  avgSteps=  4.09  avgTotalReward= 0.72\n",
            "Done: episodes=  4400  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes=  4500  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  4600  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  4700  avgSteps=  4.05  avgTotalReward= 0.72\n",
            "Done: episodes=  4800  avgSteps=  4.06  avgTotalReward= 0.72\n",
            "Done: episodes=  4900  avgSteps=  4.07  avgTotalReward= 0.72\n",
            "Done: episodes=  5000  avgSteps=  4.06  avgTotalReward= 0.72\n",
            "Done: episodes=  5100  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes=  5200  avgSteps=  4.09  avgTotalReward= 0.72\n",
            "Done: episodes=  5300  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes=  5400  avgSteps=  4.07  avgTotalReward= 0.72\n",
            "Done: episodes=  5500  avgSteps=  4.10  avgTotalReward= 0.72\n",
            "Done: episodes=  5600  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes=  5700  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  5800  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  5900  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes=  6000  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  6100  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  6200  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes=  6300  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes=  6400  avgSteps=  4.06  avgTotalReward= 0.72\n",
            "CPU times: user 6min 29s, sys: 56.8 s, total: 7min 26s\n",
            "Wall time: 5min 11s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qwdhD6D6TV2i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "353e0b00-9c96-4cf0-cb1d-9d8312b8ef92"
      },
      "cell_type": "code",
      "source": [
        "# !cat logs/berater/progress.csv"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% time spent exploring,episodes,mean 100 episode reward,steps\n",
            "54,1000,0.5,7030\n",
            "21,2000,0.6,12021\n",
            "2,3000,0.7,16232\n",
            "2,4000,0.7,20276\n",
            "2,5000,0.7,24329\n",
            "2,6000,0.7,28376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JmN8Y-xER-y4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1091a94-fc67-424a-df11-44f7a7097dea"
      },
      "cell_type": "code",
      "source": [
        "# from baselines.common import plot_util as pu\n",
        "# results = pu.load_results('/content/logs/berater')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "skipping /content/logs/berater: no monitor files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_cNrSo5FSBYy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# r = results[0]\n",
        "# plt.plot(r.progress.total_timesteps, r.progress.eprewmean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TtBh4c6-kQ4K"
      },
      "cell_type": "markdown",
      "source": [
        "# Enjoy model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ucP0gNhhkQ4O",
        "outputId": "ed97fdf5-79a7-432d-fa14-18d8a217174c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "observation = env.reset()\n",
        "state = np.zeros((1, 2*128))\n",
        "dones = np.zeros((1))\n",
        "\n",
        "BeraterEnv.showStep = True\n",
        "BeraterEnv.showDone = False\n",
        "\n",
        "for t in range(1000):\n",
        "    actions, _, state, _ = model.step(observation, S=state, M=dones)\n",
        "    observation, reward, done, info = env.step(actions[0])\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 6402   Step:    1  S --0-> A R= 0.30 totalR= 0.30 cost= 100 customerR=1000 optimum=3000\n",
            "Episode: 6402   Step:    2  A --0-> B R= 0.25 totalR= 0.55 cost= 250 customerR=1000 optimum=3000\n",
            "Episode: 6402   Step:    3  B --1-> C R= 0.25 totalR= 0.80 cost= 250 customerR=1000 optimum=3000\n",
            "Episode: 6402   Step:    4  C --2-> S R=-0.07 totalR= 0.73 cost= 200 customerR=   0 optimum=3000\n",
            "Episode finished after 4 timesteps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5fY1da_0l15E",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}