{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "berater-v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/berater-v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eU7ylMh1kQ2y"
      },
      "cell_type": "markdown",
      "source": [
        "# Berater Environment v2\n",
        "\n",
        "## Changes from v1\n",
        "1. change of observation space \n",
        "  * used to be just on discrete value: position on graph: ```spaces.Discrete(1)```\n",
        "  * give agent complete field including costs to enable learning: ```spaces.Box```\n",
        "\n",
        "## Next Steps\n",
        "1. choose costs of traversal randomly with each episode\n",
        "  * aim: agent will (hopefully) be able to work with any costs\n",
        "1. train a different graph with each episode \n",
        "  * aim: agent can work on any graph"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zpzHtN3-kQ26"
      },
      "cell_type": "markdown",
      "source": [
        "## Installation (required for colab)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0E567zPTkQ28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "49e0724e-5ce2-46c1-f31f-e0f66b9a4d98"
      },
      "cell_type": "code",
      "source": [
        "!pip install -e git+https://github.com/openai/baselines#egg=berater"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining berater from git+https://github.com/openai/baselines#egg=berater\n",
            "  Cloning https://github.com/openai/baselines to ./src/berater\n",
            "\u001b[33m  Running setup.py (path:/content/src/berater/setup.py) egg_info for package berater produced metadata for project name baselines. Fix your #egg=berater fragments.\u001b[0m\n",
            "Collecting gym (from baselines)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/22/4ff09745ade385ffe707fb5f053548f0f6a6e7d5e98a2b9d6c07f5b931a7/gym-0.10.9.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines) (4.28.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines) (0.13.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from baselines) (0.2.8.2)\n",
            "Collecting progressbar2 (from baselines)\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/6f/acb2dd76f2c77527584bd3a4c2509782bb35c481c610521fc3656de5a9e0/progressbar2-3.38.0-py2.py3-none-any.whl\n",
            "Collecting cloudpickle (from baselines)\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/87/7b7ef3038b4783911e3fdecb5c566e3a817ce3e890e164fc174c088edb1e/cloudpickle-0.6.1-py2.py3-none-any.whl\n",
            "Collecting click (from baselines)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 25.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines) (3.4.4.19)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->baselines) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym->baselines) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->baselines) (1.11.0)\n",
            "Collecting pyglet>=1.2.0 (from gym->baselines)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 18.7MB/s \n",
            "\u001b[?25hCollecting python-utils>=2.3.0 (from progressbar2->baselines)\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->baselines) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->baselines) (2018.10.15)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->baselines) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->baselines) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym->baselines) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6c/3a/0e/b86dee98876bb56cdb482cc1f72201035e46d1baf69d10d028\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym, python-utils, progressbar2, cloudpickle, click, baselines\n",
            "  Running setup.py develop for baselines\n",
            "Successfully installed baselines click-7.0 cloudpickle-0.6.1 gym-0.10.9 progressbar2-3.38.0 pyglet-1.3.2 python-utils-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2r0ZJnWJkfxL"
      },
      "cell_type": "markdown",
      "source": [
        "### important for colab: comment line above and restart runtime after installation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7Y3XKzw1kQ3I",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnt=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-S4sZG5ZkQ3T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import gym\n",
        "from gym.utils import seeding\n",
        "from gym import spaces\n",
        "\n",
        "def state_name_to_int(state):\n",
        "    state_name_map = {\n",
        "        'S': 0,\n",
        "        'A': 1,\n",
        "        'B': 2,\n",
        "        'C': 3,\n",
        "    }\n",
        "    return state_name_map[state]\n",
        "\n",
        "def int_to_state_name(state_as_int):\n",
        "    state_map = {\n",
        "        0: 'S',\n",
        "        1: 'A',\n",
        "        2: 'B',\n",
        "        3: 'C'\n",
        "    }\n",
        "    return state_map[state_as_int]\n",
        "    \n",
        "class BeraterEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    The Berater Problem\n",
        "\n",
        "    Actions: \n",
        "    There are 3 discrete deterministic actions:\n",
        "    - 0: First Direction\n",
        "    - 1: Second Direction\n",
        "    - 2: Third Direction / Go home\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['ansi']}\n",
        "    \n",
        "    num_envs = 1\n",
        "    showStep = False\n",
        "    showDone = True\n",
        "    showRender = False\n",
        "    envEpisodeModulo = 100\n",
        "\n",
        "    def __init__(self):\n",
        "        self.map = {\n",
        "            'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
        "            'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
        "            'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
        "            'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
        "        }\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(low=numpy.array([0,-1000,-1000,-1000,-1000,-1000,-1000]),\n",
        "                                             high=numpy.array([3,1000,1000,1000,1000,1000,1000]),\n",
        "                                             dtype=numpy.float32)\n",
        "\n",
        "\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.envReward = 0\n",
        "        self.envEpisodeCount = 0\n",
        "        self.envStepCount = 0\n",
        "\n",
        "        self.reset()\n",
        "        self.optimum = self.calculate_customers_reward()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, actionArg):\n",
        "        paths = self.map[self.state]\n",
        "        action = actionArg\n",
        "        destination, cost = paths[action]\n",
        "        lastState = self.state\n",
        "        lastObState = state_name_to_int(lastState)\n",
        "        customerReward = self.customer_reward[destination]\n",
        "\n",
        "        info = {\"from\": self.state, \"to\": destination}\n",
        "\n",
        "        self.state = destination\n",
        "        reward = (-cost + self.customer_reward[destination]) / self.optimum\n",
        "        self.customer_visited(destination)\n",
        "        done = destination == 'S' and self.all_customers_visited()\n",
        "\n",
        "        stateAsInt = state_name_to_int(self.state)\n",
        "        self.totalReward += reward\n",
        "        self.stepCount += 1\n",
        "        self.envReward += reward\n",
        "        self.envStepCount += 1\n",
        "\n",
        "        if self.showStep:\n",
        "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
        "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
        "                   #lastState + ':' + str(lastObState) + ' --' + str(action) + '-> ' + self.state + ':' + str(stateAsInt) +\n",
        "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
        "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
        "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
        "                   )\n",
        "\n",
        "        if done and not self.isDone:\n",
        "            self.envEpisodeCount += 1\n",
        "            if BeraterEnv.showDone or (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                episodes = BeraterEnv.envEpisodeModulo\n",
        "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
        "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
        "                print( \"Done: \" + \n",
        "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
        "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
        "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
        "                        )\n",
        "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                    self.envReward = 0\n",
        "                    self.envStepCount = 0\n",
        "\n",
        "        self.isDone = done\n",
        "        observation = self.getObservation(stateAsInt)\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def getObservation(self, position):\n",
        "        result = numpy.array([ position, \n",
        "                               self.getEdgeObservation('S','A'),\n",
        "                               self.getEdgeObservation('S','B'),\n",
        "                               self.getEdgeObservation('S','C'),\n",
        "                               self.getEdgeObservation('A','B'),\n",
        "                               self.getEdgeObservation('A','C'),\n",
        "                               self.getEdgeObservation('B','C'),\n",
        "                              ],\n",
        "                             dtype=numpy.float32)\n",
        "        return result\n",
        "\n",
        "    def getEdgeObservation(self, source, target):\n",
        "        reward = self.customer_reward[target] \n",
        "        cost = self.getCost(source,target)\n",
        "        result = reward - cost\n",
        "\n",
        "        return result\n",
        "\n",
        "    def getCost(self, source, target):\n",
        "        paths = self.map[source]\n",
        "        targetIndex=state_name_to_int(target)\n",
        "        for destination, cost in paths:\n",
        "            if destination == target:\n",
        "                result = cost\n",
        "                break\n",
        "\n",
        "        return result\n",
        "\n",
        "    def customer_visited(self, customer):\n",
        "        self.customer_reward[customer] = 0\n",
        "\n",
        "    def all_customers_visited(self):\n",
        "        return self.calculate_customers_reward() == 0\n",
        "\n",
        "    def calculate_customers_reward(self):\n",
        "        sum = 0\n",
        "        for value in self.customer_reward.values():\n",
        "            sum += value\n",
        "        return sum\n",
        "\n",
        "    def reset(self):\n",
        "        # print(\"Reset\")\n",
        "        \n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "        reward_per_customer = 1000\n",
        "        self.customer_reward = {\n",
        "            'S': 0,\n",
        "            'A': reward_per_customer,\n",
        "            'B': reward_per_customer,\n",
        "            'C': reward_per_customer,\n",
        "        }\n",
        "\n",
        "        self.state = 'S'\n",
        "        return state_name_to_int(self.state)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        if BeraterEnv.showRender:\n",
        "            print( (\"steps=%4.0f  \" % self.stepCount) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + ' done=' + str(self.isDone))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kWF5vSsakQ3b"
      },
      "cell_type": "markdown",
      "source": [
        "# Register Einvornment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SU1NxPMokQ3e",
        "outputId": "170d5c21-d6cf-41cf-de70-6098465d9b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from gym.envs.registration import register\n",
        "\n",
        "cnt += 1\n",
        "id = \"Berater-v{}\".format(cnt)\n",
        "register(\n",
        "    id=id,\n",
        "    entry_point=BeraterEnv\n",
        ")   \n",
        "\n",
        "print(id)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Berater-v1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Usj9iWTskQ3t"
      },
      "cell_type": "markdown",
      "source": [
        "# Try out Environment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oTtUfeONkQ3w",
        "outputId": "7921b563-b1a1-479c-fff7-d82ad4197322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = True\n",
        "BeraterEnv.showDone = True\n",
        "\n",
        "env = gym.make(id)\n",
        "observation = env.reset()\n",
        "print(env)\n",
        "\n",
        "for t in range(1000):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        env.render()\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BeraterEnv<Berater-v1>>\n",
            "Episode:    0   Step:    1  S --0-> A R= 0.30 totalR= 0.30 cost= 100 customerR=1000 optimum=3000\n",
            "Episode:    0   Step:    2  A --1-> C R= 0.20 totalR= 0.50 cost= 400 customerR=1000 optimum=3000\n",
            "Episode:    0   Step:    3  C --0-> A R=-0.13 totalR= 0.37 cost= 400 customerR=   0 optimum=3000\n",
            "Episode:    0   Step:    4  A --1-> C R=-0.13 totalR= 0.23 cost= 400 customerR=   0 optimum=3000\n",
            "Episode:    0   Step:    5  C --1-> B R= 0.25 totalR= 0.48 cost= 250 customerR=1000 optimum=3000\n",
            "Episode:    0   Step:    6  B --2-> S R=-0.13 totalR= 0.35 cost= 400 customerR=   0 optimum=3000\n",
            "Done: episodes=     1  avgSteps=  6.00  avgTotalReward= 0.35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4GlYjZ3xkQ38"
      },
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NzbylmYAkQ3-",
        "outputId": "f97cbaf2-b729-4955-b6f5-1a2ae066f8da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2975
        }
      },
      "cell_type": "code",
      "source": [
        "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
        "from baselines.ppo2 import ppo2\n",
        "\n",
        "BeraterEnv.showStep = False\n",
        "BeraterEnv.showDone = False\n",
        "\n",
        "wrapped_env = DummyVecEnv([lambda: gym.make(id)])\n",
        "model = ppo2.learn(network='mlp', env=wrapped_env, total_timesteps=60000)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logging to /tmp/openai-2018-12-03-16-18-54-072955\n",
            "Done: episodes=   100  avgSteps=  8.49  avgTotalReward= 0.24\n",
            "Done: episodes=   200  avgSteps=  8.85  avgTotalReward= 0.22\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0015304903 |\n",
            "| clipfrac           | 0.0061035156 |\n",
            "| eplenmean          | nan          |\n",
            "| eprewmean          | nan          |\n",
            "| explained_variance | -0.45        |\n",
            "| fps                | 436          |\n",
            "| nupdates           | 1            |\n",
            "| policy_entropy     | 1.0969917    |\n",
            "| policy_loss        | -0.010166587 |\n",
            "| serial_timesteps   | 2048         |\n",
            "| time_elapsed       | 4.69         |\n",
            "| total_timesteps    | 2048         |\n",
            "| value_loss         | 0.04548084   |\n",
            "-------------------------------------\n",
            "Done: episodes=   300  avgSteps=  8.23  avgTotalReward= 0.25\n",
            "Done: episodes=   400  avgSteps=  7.49  avgTotalReward= 0.34\n",
            "Done: episodes=   500  avgSteps=  7.42  avgTotalReward= 0.33\n",
            "Done: episodes=   600  avgSteps=  7.14  avgTotalReward= 0.37\n",
            "Done: episodes=   700  avgSteps=  7.54  avgTotalReward= 0.32\n",
            "Done: episodes=   800  avgSteps=  7.03  avgTotalReward= 0.38\n",
            "Done: episodes=   900  avgSteps=  7.07  avgTotalReward= 0.38\n",
            "Done: episodes=  1000  avgSteps=  6.82  avgTotalReward= 0.40\n",
            "Done: episodes=  1100  avgSteps=  6.73  avgTotalReward= 0.40\n",
            "Done: episodes=  1200  avgSteps=  6.03  avgTotalReward= 0.46\n",
            "Done: episodes=  1300  avgSteps=  6.40  avgTotalReward= 0.44\n",
            "Done: episodes=  1400  avgSteps=  6.75  avgTotalReward= 0.40\n",
            "Done: episodes=  1500  avgSteps=  5.77  avgTotalReward= 0.49\n",
            "Done: episodes=  1600  avgSteps=  5.95  avgTotalReward= 0.46\n",
            "Done: episodes=  1700  avgSteps=  5.74  avgTotalReward= 0.49\n",
            "Done: episodes=  1800  avgSteps=  5.86  avgTotalReward= 0.47\n",
            "Done: episodes=  1900  avgSteps=  5.48  avgTotalReward= 0.51\n",
            "Done: episodes=  2000  avgSteps=  5.86  avgTotalReward= 0.46\n",
            "Done: episodes=  2100  avgSteps=  5.29  avgTotalReward= 0.53\n",
            "Done: episodes=  2200  avgSteps=  5.21  avgTotalReward= 0.55\n",
            "Done: episodes=  2300  avgSteps=  5.21  avgTotalReward= 0.54\n",
            "Done: episodes=  2400  avgSteps=  4.98  avgTotalReward= 0.56\n",
            "Done: episodes=  2500  avgSteps=  5.50  avgTotalReward= 0.51\n",
            "Done: episodes=  2600  avgSteps=  4.81  avgTotalReward= 0.58\n",
            "Done: episodes=  2700  avgSteps=  4.84  avgTotalReward= 0.57\n",
            "Done: episodes=  2800  avgSteps=  4.97  avgTotalReward= 0.54\n",
            "Done: episodes=  2900  avgSteps=  4.86  avgTotalReward= 0.56\n",
            "Done: episodes=  3000  avgSteps=  4.80  avgTotalReward= 0.56\n",
            "Done: episodes=  3100  avgSteps=  4.43  avgTotalReward= 0.60\n",
            "Done: episodes=  3200  avgSteps=  4.54  avgTotalReward= 0.59\n",
            "Done: episodes=  3300  avgSteps=  4.57  avgTotalReward= 0.60\n",
            "-------------------------------------\n",
            "| approxkl           | 0.0017361455 |\n",
            "| clipfrac           | 0.017944336  |\n",
            "| eplenmean          | nan          |\n",
            "| eprewmean          | nan          |\n",
            "| explained_variance | 0.876        |\n",
            "| fps                | 521          |\n",
            "| nupdates           | 10           |\n",
            "| policy_entropy     | 0.68430805   |\n",
            "| policy_loss        | -0.019066116 |\n",
            "| serial_timesteps   | 20480        |\n",
            "| time_elapsed       | 39.7         |\n",
            "| total_timesteps    | 20480        |\n",
            "| value_loss         | 0.0043647513 |\n",
            "-------------------------------------\n",
            "Done: episodes=  3400  avgSteps=  4.46  avgTotalReward= 0.61\n",
            "Done: episodes=  3500  avgSteps=  4.45  avgTotalReward= 0.61\n",
            "Done: episodes=  3600  avgSteps=  4.40  avgTotalReward= 0.63\n",
            "Done: episodes=  3700  avgSteps=  4.25  avgTotalReward= 0.63\n",
            "Done: episodes=  3800  avgSteps=  4.50  avgTotalReward= 0.60\n",
            "Done: episodes=  3900  avgSteps=  4.44  avgTotalReward= 0.62\n",
            "Done: episodes=  4000  avgSteps=  4.42  avgTotalReward= 0.64\n",
            "Done: episodes=  4100  avgSteps=  4.34  avgTotalReward= 0.63\n",
            "Done: episodes=  4200  avgSteps=  4.32  avgTotalReward= 0.63\n",
            "Done: episodes=  4300  avgSteps=  4.47  avgTotalReward= 0.61\n",
            "Done: episodes=  4400  avgSteps=  4.27  avgTotalReward= 0.64\n",
            "Done: episodes=  4500  avgSteps=  4.37  avgTotalReward= 0.63\n",
            "Done: episodes=  4600  avgSteps=  4.19  avgTotalReward= 0.66\n",
            "Done: episodes=  4700  avgSteps=  4.24  avgTotalReward= 0.64\n",
            "Done: episodes=  4800  avgSteps=  4.26  avgTotalReward= 0.65\n",
            "Done: episodes=  4900  avgSteps=  4.19  avgTotalReward= 0.67\n",
            "Done: episodes=  5000  avgSteps=  4.22  avgTotalReward= 0.66\n",
            "Done: episodes=  5100  avgSteps=  4.17  avgTotalReward= 0.67\n",
            "Done: episodes=  5200  avgSteps=  4.28  avgTotalReward= 0.65\n",
            "Done: episodes=  5300  avgSteps=  4.13  avgTotalReward= 0.68\n",
            "Done: episodes=  5400  avgSteps=  4.11  avgTotalReward= 0.67\n",
            "Done: episodes=  5500  avgSteps=  4.15  avgTotalReward= 0.67\n",
            "Done: episodes=  5600  avgSteps=  4.17  avgTotalReward= 0.67\n",
            "Done: episodes=  5700  avgSteps=  4.18  avgTotalReward= 0.68\n",
            "Done: episodes=  5800  avgSteps=  4.12  avgTotalReward= 0.68\n",
            "Done: episodes=  5900  avgSteps=  4.05  avgTotalReward= 0.68\n",
            "Done: episodes=  6000  avgSteps=  4.12  avgTotalReward= 0.68\n",
            "Done: episodes=  6100  avgSteps=  4.09  avgTotalReward= 0.69\n",
            "Done: episodes=  6200  avgSteps=  4.14  avgTotalReward= 0.68\n",
            "Done: episodes=  6300  avgSteps=  4.09  avgTotalReward= 0.69\n",
            "Done: episodes=  6400  avgSteps=  4.15  avgTotalReward= 0.70\n",
            "Done: episodes=  6500  avgSteps=  4.05  avgTotalReward= 0.71\n",
            "Done: episodes=  6600  avgSteps=  4.11  avgTotalReward= 0.69\n",
            "Done: episodes=  6700  avgSteps=  4.09  avgTotalReward= 0.71\n",
            "Done: episodes=  6800  avgSteps=  4.13  avgTotalReward= 0.69\n",
            "Done: episodes=  6900  avgSteps=  4.02  avgTotalReward= 0.71\n",
            "Done: episodes=  7000  avgSteps=  4.04  avgTotalReward= 0.71\n",
            "Done: episodes=  7100  avgSteps=  4.07  avgTotalReward= 0.72\n",
            "Done: episodes=  7200  avgSteps=  4.07  avgTotalReward= 0.71\n",
            "Done: episodes=  7300  avgSteps=  4.09  avgTotalReward= 0.71\n",
            "Done: episodes=  7400  avgSteps=  4.03  avgTotalReward= 0.72\n",
            "Done: episodes=  7500  avgSteps=  4.07  avgTotalReward= 0.72\n",
            "Done: episodes=  7600  avgSteps=  4.05  avgTotalReward= 0.71\n",
            "Done: episodes=  7700  avgSteps=  4.04  avgTotalReward= 0.72\n",
            "Done: episodes=  7800  avgSteps=  4.07  avgTotalReward= 0.72\n",
            "Done: episodes=  7900  avgSteps=  4.04  avgTotalReward= 0.72\n",
            "Done: episodes=  8000  avgSteps=  4.04  avgTotalReward= 0.71\n",
            "Done: episodes=  8100  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes=  8200  avgSteps=  4.05  avgTotalReward= 0.72\n",
            "Done: episodes=  8300  avgSteps=  4.04  avgTotalReward= 0.72\n",
            "--------------------------------------\n",
            "| approxkl           | 0.00043752656 |\n",
            "| clipfrac           | 0.004638672   |\n",
            "| eplenmean          | nan           |\n",
            "| eprewmean          | nan           |\n",
            "| explained_variance | 0.985         |\n",
            "| fps                | 521           |\n",
            "| nupdates           | 20            |\n",
            "| policy_entropy     | 0.1696136     |\n",
            "| policy_loss        | -0.0118810395 |\n",
            "| serial_timesteps   | 40960         |\n",
            "| time_elapsed       | 78.5          |\n",
            "| total_timesteps    | 40960         |\n",
            "| value_loss         | 0.000630336   |\n",
            "--------------------------------------\n",
            "Done: episodes=  8400  avgSteps=  4.01  avgTotalReward= 0.72\n",
            "Done: episodes=  8500  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes=  8600  avgSteps=  4.06  avgTotalReward= 0.72\n",
            "Done: episodes=  8700  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes=  8800  avgSteps=  4.06  avgTotalReward= 0.72\n",
            "Done: episodes=  8900  avgSteps=  4.03  avgTotalReward= 0.72\n",
            "Done: episodes=  9000  avgSteps=  4.03  avgTotalReward= 0.72\n",
            "Done: episodes=  9100  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes=  9200  avgSteps=  4.03  avgTotalReward= 0.72\n",
            "Done: episodes=  9300  avgSteps=  4.05  avgTotalReward= 0.72\n",
            "Done: episodes=  9400  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes=  9500  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes=  9600  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes=  9700  avgSteps=  4.00  avgTotalReward= 0.72\n",
            "Done: episodes=  9800  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes=  9900  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes= 10000  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes= 10100  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes= 10200  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes= 10300  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes= 10400  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes= 10500  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes= 10600  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes= 10700  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes= 10800  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes= 10900  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes= 11000  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes= 11100  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes= 11200  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes= 11300  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes= 11400  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes= 11500  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes= 11600  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes= 11700  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes= 11800  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes= 11900  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes= 12000  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes= 12100  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes= 12200  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes= 12300  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes= 12400  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes= 12500  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes= 12600  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes= 12700  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes= 12800  avgSteps=  4.02  avgTotalReward= 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TtBh4c6-kQ4K"
      },
      "cell_type": "markdown",
      "source": [
        "# Enjoy model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ucP0gNhhkQ4O",
        "outputId": "6dc27524-6a4d-4eb1-c156-61cc81e083be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "observation = wrapped_env.reset()\n",
        "state = np.zeros((1, 2*128))\n",
        "dones = np.zeros((1))\n",
        "\n",
        "BeraterEnv.showStep = True\n",
        "BeraterEnv.showDone = False\n",
        "\n",
        "for t in range(1000):\n",
        "    actions, _, state, _ = model.step(observation, S=state, M=dones)\n",
        "    observation, reward, done, info = wrapped_env.step(actions)\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 12890   Step:    1  S --2-> C R= 0.27 totalR= 0.27 cost= 200 customerR=1000 optimum=3000\n",
            "Episode: 12890   Step:    2  C --1-> B R= 0.25 totalR= 0.52 cost= 250 customerR=1000 optimum=3000\n",
            "Episode: 12890   Step:    3  B --0-> A R= 0.25 totalR= 0.77 cost= 250 customerR=1000 optimum=3000\n",
            "Episode: 12890   Step:    4  A --2-> S R=-0.03 totalR= 0.73 cost= 100 customerR=   0 optimum=3000\n",
            "Episode finished after 4 timesteps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5fY1da_0l15E",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}