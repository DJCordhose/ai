{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils import seeding\n",
    "from gym import spaces\n",
    "\n",
    "def state_name_to_int(state):\n",
    "    state_name_map = {\n",
    "        'S/Z': 0,\n",
    "        'A': 1,\n",
    "        'B': 2,\n",
    "        'C': 3,\n",
    "    }\n",
    "    return state_name_map[state]\n",
    "\n",
    "def int_to_state_name(state_as_int):\n",
    "    state_map = {\n",
    "        0: 'S/Z',\n",
    "        1: 'A',\n",
    "        2: 'B',\n",
    "        3: 'C'\n",
    "    }\n",
    "    return state_map[state_as_int]\n",
    "    \n",
    "class BeraterEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    The Berater Problem\n",
    "\n",
    "    Actions: \n",
    "    There are 3 discrete deterministic actions:\n",
    "    - 0: First Direction\n",
    "    - 1: Second Direction\n",
    "    - 2: Third Direction / Go home\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['ansi']}\n",
    "    \n",
    "    num_envs = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        self.map = {\n",
    "            'S/Z': [('A', 100), ('B', 400), ('C', 200 )],\n",
    "            'A': [('B', 250), ('C', 400), ('S/Z', 100 )],\n",
    "            'B': [('A', 250), ('C', 250), ('S/Z', 400 )],\n",
    "            'C': [('A', 400), ('B', 250), ('S/Z', 200 )]\n",
    "        }\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Discrete(1)\n",
    "\n",
    "        self.reset()\n",
    "        self.optimum = self.calculate_customers_reward()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        paths = self.map[self.state]\n",
    "        destination, cost = paths[action]\n",
    "\n",
    "        info = {\"from\": self.state, \"to\": destination}\n",
    "\n",
    "        self.state = destination\n",
    "        reward = (-cost + self.customer_reward[destination]) / self.optimum\n",
    "        self.customer_visited(destination)\n",
    "        done = destination == 'S/Z' and self.all_customers_visited()\n",
    "        return state_name_to_int(self.state), reward, done, info\n",
    "\n",
    "    def customer_visited(self, customer):\n",
    "        self.customer_reward[customer] = 0\n",
    "\n",
    "    def all_customers_visited(self):\n",
    "        return self.calculate_customers_reward() == 0\n",
    "\n",
    "    def calculate_customers_reward(self):\n",
    "        sum = 0\n",
    "        for value in self.customer_reward.values():\n",
    "            sum += value\n",
    "        return sum\n",
    "\n",
    "    def reset(self):\n",
    "        # print(\"Reset\")\n",
    "        reward_per_customer = 1000\n",
    "        self.customer_reward = {\n",
    "            'S/Z': 0,\n",
    "            'A': reward_per_customer,\n",
    "            'B': reward_per_customer,\n",
    "            'C': reward_per_customer,\n",
    "        }\n",
    "\n",
    "        self.state = 'S/Z'\n",
    "        return state_name_to_int(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(self.state)\n",
    "        print(self.customer_reward)\n",
    "\n",
    "from gym.envs.registration import register\n",
    "cnt += 1\n",
    "id = \"Berater-v{}\".format(cnt)\n",
    "register(\n",
    "    id=id,\n",
    "    entry_point=BeraterEnv\n",
    ")        \n",
    "env = gym.make(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/Z\n",
      "{'S/Z': 0, 'A': 1000, 'B': 1000, 'C': 1000}\n",
      "Action: 0\n",
      "0.3\n",
      "A\n",
      "{'S/Z': 0, 'A': 0, 'B': 1000, 'C': 1000}\n",
      "Action: 1\n",
      "0.2\n",
      "C\n",
      "{'S/Z': 0, 'A': 0, 'B': 1000, 'C': 0}\n",
      "Action: 0\n",
      "-0.13333333333333333\n",
      "A\n",
      "{'S/Z': 0, 'A': 0, 'B': 1000, 'C': 0}\n",
      "Action: 1\n",
      "-0.13333333333333333\n",
      "C\n",
      "{'S/Z': 0, 'A': 0, 'B': 1000, 'C': 0}\n",
      "Action: 1\n",
      "0.25\n",
      "B\n",
      "{'S/Z': 0, 'A': 0, 'B': 0, 'C': 0}\n",
      "Action: 2\n",
      "-0.13333333333333333\n",
      "Episode finished after 6 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Run a demo of the environment\n",
    "observation = env.reset()\n",
    "for t in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    print(\"Action: {}\".format(action))\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(reward)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train PPO2 using MLP Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to C:\\Users\\olive\\AppData\\Local\\Temp\\openai-2018-11-02-19-22-26-907765\n",
      "--------------------------------------\n",
      "| approxkl           | 6.1440616e-05 |\n",
      "| clipfrac           | 0.0           |\n",
      "| eplenmean          | nan           |\n",
      "| eprewmean          | nan           |\n",
      "| explained_variance | 0.158         |\n",
      "| fps                | 533           |\n",
      "| nupdates           | 1             |\n",
      "| policy_entropy     | 1.0985489     |\n",
      "| policy_loss        | -0.0011003441 |\n",
      "| serial_timesteps   | 2048          |\n",
      "| time_elapsed       | 3.84          |\n",
      "| total_timesteps    | 2048          |\n",
      "| value_loss         | 0.04378471    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00036273547 |\n",
      "| clipfrac           | 0.0           |\n",
      "| eplenmean          | nan           |\n",
      "| eprewmean          | nan           |\n",
      "| explained_variance | 0.13          |\n",
      "| fps                | 512           |\n",
      "| nupdates           | 10            |\n",
      "| policy_entropy     | 1.0540723     |\n",
      "| policy_loss        | -0.0013715331 |\n",
      "| serial_timesteps   | 20480         |\n",
      "| time_elapsed       | 43.2          |\n",
      "| total_timesteps    | 20480         |\n",
      "| value_loss         | 0.039634086   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| approxkl           | 0.00015908993  |\n",
      "| clipfrac           | 0.0            |\n",
      "| eplenmean          | nan            |\n",
      "| eprewmean          | nan            |\n",
      "| explained_variance | 0.108          |\n",
      "| fps                | 575            |\n",
      "| nupdates           | 20             |\n",
      "| policy_entropy     | 1.0021784      |\n",
      "| policy_loss        | -0.00018330014 |\n",
      "| serial_timesteps   | 40960          |\n",
      "| time_elapsed       | 78.3           |\n",
      "| total_timesteps    | 40960          |\n",
      "| value_loss         | 0.02913522     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0001905164  |\n",
      "| clipfrac           | 0.0           |\n",
      "| eplenmean          | nan           |\n",
      "| eprewmean          | nan           |\n",
      "| explained_variance | 0.145         |\n",
      "| fps                | 544           |\n",
      "| nupdates           | 30            |\n",
      "| policy_entropy     | 1.0057396     |\n",
      "| policy_loss        | -0.0005003403 |\n",
      "| serial_timesteps   | 61440         |\n",
      "| time_elapsed       | 115           |\n",
      "| total_timesteps    | 61440         |\n",
      "| value_loss         | 0.026083676   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0005350411 |\n",
      "| clipfrac           | 0.0          |\n",
      "| eplenmean          | nan          |\n",
      "| eprewmean          | nan          |\n",
      "| explained_variance | 0.113        |\n",
      "| fps                | 607          |\n",
      "| nupdates           | 40           |\n",
      "| policy_entropy     | 1.0212185    |\n",
      "| policy_loss        | -0.000670255 |\n",
      "| serial_timesteps   | 81920        |\n",
      "| time_elapsed       | 150          |\n",
      "| total_timesteps    | 81920        |\n",
      "| value_loss         | 0.028561689  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00025252101 |\n",
      "| clipfrac           | 0.0           |\n",
      "| eplenmean          | nan           |\n",
      "| eprewmean          | nan           |\n",
      "| explained_variance | 0.0581        |\n",
      "| fps                | 541           |\n",
      "| nupdates           | 50            |\n",
      "| policy_entropy     | 0.97488844    |\n",
      "| policy_loss        | -0.0012810077 |\n",
      "| serial_timesteps   | 102400        |\n",
      "| time_elapsed       | 187           |\n",
      "| total_timesteps    | 102400        |\n",
      "| value_loss         | 0.03161272    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00019344786 |\n",
      "| clipfrac           | 0.0           |\n",
      "| eplenmean          | nan           |\n",
      "| eprewmean          | nan           |\n",
      "| explained_variance | 0.104         |\n",
      "| fps                | 626           |\n",
      "| nupdates           | 60            |\n",
      "| policy_entropy     | 1.0019867     |\n",
      "| policy_loss        | -0.0006568098 |\n",
      "| serial_timesteps   | 122880        |\n",
      "| time_elapsed       | 221           |\n",
      "| total_timesteps    | 122880        |\n",
      "| value_loss         | 0.028860534   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| approxkl           | 5.143531e-05   |\n",
      "| clipfrac           | 0.0            |\n",
      "| eplenmean          | nan            |\n",
      "| eprewmean          | nan            |\n",
      "| explained_variance | 0.118          |\n",
      "| fps                | 605            |\n",
      "| nupdates           | 70             |\n",
      "| policy_entropy     | 0.99186224     |\n",
      "| policy_loss        | -0.00012046553 |\n",
      "| serial_timesteps   | 143360         |\n",
      "| time_elapsed       | 255            |\n",
      "| total_timesteps    | 143360         |\n",
      "| value_loss         | 0.02781716     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 2.83036e-05   |\n",
      "| clipfrac           | 0.0           |\n",
      "| eplenmean          | nan           |\n",
      "| eprewmean          | nan           |\n",
      "| explained_variance | 0.113         |\n",
      "| fps                | 362           |\n",
      "| nupdates           | 80            |\n",
      "| policy_entropy     | 0.9704946     |\n",
      "| policy_loss        | 0.00020469402 |\n",
      "| serial_timesteps   | 163840        |\n",
      "| time_elapsed       | 295           |\n",
      "| total_timesteps    | 163840        |\n",
      "| value_loss         | 0.030583616   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00017448186 |\n",
      "| clipfrac           | 0.0           |\n",
      "| eplenmean          | nan           |\n",
      "| eprewmean          | nan           |\n",
      "| explained_variance | 0.131         |\n",
      "| fps                | 702           |\n",
      "| nupdates           | 90            |\n",
      "| policy_entropy     | 0.9756962     |\n",
      "| policy_loss        | 0.00015807807 |\n",
      "| serial_timesteps   | 184320        |\n",
      "| time_elapsed       | 331           |\n",
      "| total_timesteps    | 184320        |\n",
      "| value_loss         | 0.026194973   |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from baselines.ppo2 import ppo2\n",
    "\n",
    "wrapped_env = DummyVecEnv([lambda: gym.make(id)])\n",
    "\n",
    "model = ppo2.learn(network='mlp', env=wrapped_env, total_timesteps=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use trained model to do actual planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/Z\n",
      "{'S/Z': 0, 'A': 1000, 'B': 1000, 'C': 1000}\n",
      "Action: [0]\n",
      "[0.3]\n",
      "A\n",
      "{'S/Z': 0, 'A': 0, 'B': 1000, 'C': 1000}\n",
      "Action: [0]\n",
      "[0.25]\n",
      "B\n",
      "{'S/Z': 0, 'A': 0, 'B': 0, 'C': 1000}\n",
      "Action: [1]\n",
      "[0.25]\n",
      "C\n",
      "{'S/Z': 0, 'A': 0, 'B': 0, 'C': 0}\n",
      "Action: [2]\n",
      "[-0.06666667]\n",
      "Episode finished after 4 timesteps\n"
     ]
    }
   ],
   "source": [
    "observation = wrapped_env.reset()\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "state = np.zeros((1, 2*128))\n",
    "dones = np.zeros((1))\n",
    "\n",
    "for t in range(1000):\n",
    "    wrapped_env.render()\n",
    "    actions, _, state, _ = model.step(observation, S=state, M=dones)\n",
    "    print(\"Action: {}\".format(actions))\n",
    "    observation, reward, done, info = wrapped_env.step(actions)\n",
    "    print(reward)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "wrapped_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
