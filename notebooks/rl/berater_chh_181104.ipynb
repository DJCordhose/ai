{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berater Environment\n",
    "\n",
    "* der observation space besteht aus 1 Wert: self.observation_space = spaces.Discrete(1). Ich nehme an dieser soll die aktuelle position auf dem graphen darstellen (S,A,B,C). Hierfür würde ich spaces.Discrete(4) verwenden.\n",
    "\n",
    "* mit diesem observation space «weiss» der agent lediglich die position, ansonsten aber nichts ueber das restliche «spielfeld». Ich würde hier dem agent das ganze spielfeld «geben»\n",
    "\n",
    "* als policy wird hier die «default» «mlp» policy verwendet. Soweit ich sehen kann ist das ein fully-connected 2 layer nn mit 64 neuronen pro layer.\n",
    "\n",
    "\n",
    "Ich habe mal versucht, das «mit dem ganzen spielfeld» im notebook durchzuspielen. Anbei meine aktualisierte version. Das Training scheint nach <=60k steps fertig zu sein und erreicht jeweils einen durchschnittlichen total reward von ~0.73. wenn ich das von hand rechne komme ich auf einen aehnlichen wert. schau’s dir doch mal an.\n",
    "\n",
    "Aktuell sind die kosten auf den kanten des graphen fix. Interessant könnte sein, während jeder episode diese neu (random) zu wählen. Damit wäre der trainierte agent dann nach dem lernen vielleicht in der lage bei «beliebigen» Kosten jeweils eine gute lösung zu finden.\n",
    "\n",
    "habe ich verschiedene policy architekturen durchprobiert. Das waren alles policies vom «mlp» typ mit 1-5 layern und 100-4500 neuronen pro layer. Am schluss habe ich dann diejenige genommen die am «besten» und am «einfachsten» war: 1 layer, 500 neuronen mit tanh als aktivierungs-funkction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation (required for colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e git+https://github.com/openai/baselines#egg=berater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "from gym import spaces\n",
    "\n",
    "def state_name_to_int(state):\n",
    "    state_name_map = {\n",
    "        'S': 0,\n",
    "        'A': 1,\n",
    "        'B': 2,\n",
    "        'C': 3,\n",
    "    }\n",
    "    return state_name_map[state]\n",
    "\n",
    "def int_to_state_name(state_as_int):\n",
    "    state_map = {\n",
    "        0: 'S',\n",
    "        1: 'A',\n",
    "        2: 'B',\n",
    "        3: 'C'\n",
    "    }\n",
    "    return state_map[state_as_int]\n",
    "    \n",
    "class BeraterEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    The Berater Problem\n",
    "\n",
    "    Actions: \n",
    "    There are 3 discrete deterministic actions:\n",
    "    - 0: First Direction\n",
    "    - 1: Second Direction\n",
    "    - 2: Third Direction / Go home\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['ansi']}\n",
    "    \n",
    "    num_envs = 1\n",
    "    showStep = False\n",
    "    showDone = True\n",
    "    showRender = False\n",
    "    envEpisodeModulo = 100\n",
    "\n",
    "    def __init__(self):\n",
    "        self.map = {\n",
    "            'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
    "            'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
    "            'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
    "            'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
    "        }\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(low=numpy.array([0,-1000,-1000,-1000,-1000,-1000,-1000]),\n",
    "                                             high=numpy.array([3,1000,1000,1000,1000,1000,1000]),\n",
    "                                             dtype=numpy.float32)\n",
    "\n",
    "\n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "\n",
    "        self.envReward = 0\n",
    "        self.envEpisodeCount = 0\n",
    "        self.envStepCount = 0\n",
    "\n",
    "        self.reset()\n",
    "        self.optimum = self.calculate_customers_reward()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, actionArg):\n",
    "        paths = self.map[self.state]\n",
    "        action = actionArg\n",
    "        destination, cost = paths[action]\n",
    "        lastState = self.state\n",
    "        lastObState = state_name_to_int(lastState)\n",
    "        customerReward = self.customer_reward[destination]\n",
    "\n",
    "        info = {\"from\": self.state, \"to\": destination}\n",
    "\n",
    "        self.state = destination\n",
    "        reward = (-cost + self.customer_reward[destination]) / self.optimum\n",
    "        self.customer_visited(destination)\n",
    "        done = destination == 'S' and self.all_customers_visited()\n",
    "\n",
    "        stateAsInt = state_name_to_int(self.state)\n",
    "        self.totalReward += reward\n",
    "        self.stepCount += 1\n",
    "        self.envReward += reward\n",
    "        self.envStepCount += 1\n",
    "\n",
    "        if self.showStep:\n",
    "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
    "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
    "                   #lastState + ':' + str(lastObState) + ' --' + str(action) + '-> ' + self.state + ':' + str(stateAsInt) +\n",
    "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
    "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
    "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
    "                   )\n",
    "\n",
    "        if done and not self.isDone:\n",
    "            self.envEpisodeCount += 1\n",
    "            if BeraterEnv.showDone or (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
    "                episodes = BeraterEnv.envEpisodeModulo\n",
    "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
    "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
    "                print( \"Done: \" + \n",
    "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
    "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
    "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
    "                        )\n",
    "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
    "                    self.envReward = 0\n",
    "                    self.envStepCount = 0\n",
    "\n",
    "        self.isDone = done\n",
    "        observation = self.getObservation(stateAsInt)\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def getObservation(self, position):\n",
    "        result = numpy.array([ position, \n",
    "                               self.getEdgeObservation('S','A'),\n",
    "                               self.getEdgeObservation('S','B'),\n",
    "                               self.getEdgeObservation('S','C'),\n",
    "                               self.getEdgeObservation('A','B'),\n",
    "                               self.getEdgeObservation('A','C'),\n",
    "                               self.getEdgeObservation('B','C'),\n",
    "                              ],\n",
    "                             dtype=numpy.float32)\n",
    "        return result\n",
    "\n",
    "    def getEdgeObservation(self, source, target):\n",
    "        reward = self.customer_reward[target] \n",
    "        cost = self.getCost(source,target)\n",
    "        result = reward - cost\n",
    "\n",
    "        return result\n",
    "\n",
    "    def getCost(self, source, target):\n",
    "        paths = self.map[source]\n",
    "        targetIndex=state_name_to_int(target)\n",
    "        for destination, cost in paths:\n",
    "            if destination == target:\n",
    "                result = cost\n",
    "                break\n",
    "\n",
    "        return result\n",
    "\n",
    "    def customer_visited(self, customer):\n",
    "        self.customer_reward[customer] = 0\n",
    "\n",
    "    def all_customers_visited(self):\n",
    "        return self.calculate_customers_reward() == 0\n",
    "\n",
    "    def calculate_customers_reward(self):\n",
    "        sum = 0\n",
    "        for value in self.customer_reward.values():\n",
    "            sum += value\n",
    "        return sum\n",
    "\n",
    "    def reset(self):\n",
    "        # print(\"Reset\")\n",
    "        \n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "        reward_per_customer = 1000\n",
    "        self.customer_reward = {\n",
    "            'S': 0,\n",
    "            'A': reward_per_customer,\n",
    "            'B': reward_per_customer,\n",
    "            'C': reward_per_customer,\n",
    "        }\n",
    "\n",
    "        self.state = 'S'\n",
    "        return state_name_to_int(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if BeraterEnv.showRender:\n",
    "            print( (\"steps=%4.0f  \" % self.stepCount) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + ' done=' + str(self.isDone))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register Einvornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berater-v1\n"
     ]
    }
   ],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "cnt += 1\n",
    "id = \"Berater-v{}\".format(cnt)\n",
    "register(\n",
    "    id=id,\n",
    "    entry_point=BeraterEnv\n",
    ")   \n",
    "\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try out Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BeraterEnv<Berater-v1>>\n",
      "Episode:    0   Step:    1  S --0-> A R= 0.30 totalR= 0.30 cost= 100 customerR=1000 optimum=3000\n",
      "Episode:    0   Step:    2  A --1-> C R= 0.20 totalR= 0.50 cost= 400 customerR=1000 optimum=3000\n",
      "Episode:    0   Step:    3  C --0-> A R=-0.13 totalR= 0.37 cost= 400 customerR=   0 optimum=3000\n",
      "Episode:    0   Step:    4  A --1-> C R=-0.13 totalR= 0.23 cost= 400 customerR=   0 optimum=3000\n",
      "Episode:    0   Step:    5  C --1-> B R= 0.25 totalR= 0.48 cost= 250 customerR=1000 optimum=3000\n",
      "Episode:    0   Step:    6  B --2-> S R=-0.13 totalR= 0.35 cost= 400 customerR=   0 optimum=3000\n",
      "Done: episodes=     1  avgSteps=  6.00  avgTotalReward= 0.35\n"
     ]
    }
   ],
   "source": [
    "BeraterEnv.showStep = True\n",
    "BeraterEnv.showDone = True\n",
    "\n",
    "env = gym.make(id)\n",
    "observation = env.reset()\n",
    "print(env)\n",
    "\n",
    "for t in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.render()\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to C:\\Users\\olive\\AppData\\Local\\Temp\\openai-2018-11-26-10-49-39-067313\n",
      "Done: episodes=   100  avgSteps=  8.50  avgTotalReward= 0.24\n",
      "Done: episodes=   200  avgSteps=  8.69  avgTotalReward= 0.23\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0011750747 |\n",
      "| clipfrac           | 0.0          |\n",
      "| eplenmean          | nan          |\n",
      "| eprewmean          | nan          |\n",
      "| explained_variance | -3.87        |\n",
      "| fps                | 233          |\n",
      "| nupdates           | 1            |\n",
      "| policy_entropy     | 1.0973399    |\n",
      "| policy_loss        | -0.00390506  |\n",
      "| serial_timesteps   | 2048         |\n",
      "| time_elapsed       | 8.78         |\n",
      "| total_timesteps    | 2048         |\n",
      "| value_loss         | 0.13186155   |\n",
      "-------------------------------------\n",
      "Done: episodes=   300  avgSteps=  7.48  avgTotalReward= 0.34\n",
      "Done: episodes=   400  avgSteps=  7.84  avgTotalReward= 0.30\n",
      "Done: episodes=   500  avgSteps=  7.76  avgTotalReward= 0.31\n",
      "Done: episodes=   600  avgSteps=  7.39  avgTotalReward= 0.34\n",
      "Done: episodes=   700  avgSteps=  7.19  avgTotalReward= 0.36\n",
      "Done: episodes=   800  avgSteps=  6.82  avgTotalReward= 0.41\n",
      "Done: episodes=   900  avgSteps=  6.78  avgTotalReward= 0.40\n",
      "Done: episodes=  1000  avgSteps=  6.67  avgTotalReward= 0.41\n",
      "Done: episodes=  1100  avgSteps=  6.86  avgTotalReward= 0.39\n",
      "Done: episodes=  1200  avgSteps=  6.11  avgTotalReward= 0.46\n",
      "Done: episodes=  1300  avgSteps=  6.26  avgTotalReward= 0.44\n",
      "Done: episodes=  1400  avgSteps=  6.00  avgTotalReward= 0.49\n",
      "Done: episodes=  1500  avgSteps=  5.87  avgTotalReward= 0.47\n",
      "Done: episodes=  1600  avgSteps=  5.55  avgTotalReward= 0.49\n",
      "Done: episodes=  1700  avgSteps=  5.82  avgTotalReward= 0.48\n",
      "Done: episodes=  1800  avgSteps=  6.24  avgTotalReward= 0.45\n",
      "Done: episodes=  1900  avgSteps=  5.37  avgTotalReward= 0.51\n",
      "Done: episodes=  2000  avgSteps=  5.26  avgTotalReward= 0.52\n",
      "Done: episodes=  2100  avgSteps=  5.66  avgTotalReward= 0.49\n",
      "Done: episodes=  2200  avgSteps=  5.55  avgTotalReward= 0.49\n",
      "Done: episodes=  2300  avgSteps=  5.40  avgTotalReward= 0.53\n",
      "Done: episodes=  2400  avgSteps=  5.20  avgTotalReward= 0.55\n",
      "Done: episodes=  2500  avgSteps=  5.21  avgTotalReward= 0.54\n",
      "Done: episodes=  2600  avgSteps=  5.29  avgTotalReward= 0.53\n",
      "Done: episodes=  2700  avgSteps=  4.99  avgTotalReward= 0.55\n",
      "Done: episodes=  2800  avgSteps=  4.91  avgTotalReward= 0.56\n",
      "Done: episodes=  2900  avgSteps=  5.06  avgTotalReward= 0.53\n",
      "Done: episodes=  3000  avgSteps=  5.02  avgTotalReward= 0.56\n",
      "Done: episodes=  3100  avgSteps=  4.81  avgTotalReward= 0.57\n",
      "Done: episodes=  3200  avgSteps=  4.87  avgTotalReward= 0.57\n",
      "Done: episodes=  3300  avgSteps=  5.06  avgTotalReward= 0.54\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0022457    |\n",
      "| clipfrac           | 0.019897461  |\n",
      "| eplenmean          | nan          |\n",
      "| eprewmean          | nan          |\n",
      "| explained_variance | 0.761        |\n",
      "| fps                | 245          |\n",
      "| nupdates           | 10           |\n",
      "| policy_entropy     | 0.7562346    |\n",
      "| policy_loss        | -0.017888635 |\n",
      "| serial_timesteps   | 20480        |\n",
      "| time_elapsed       | 81.2         |\n",
      "| total_timesteps    | 20480        |\n",
      "| value_loss         | 0.008641124  |\n",
      "-------------------------------------\n",
      "Done: episodes=  3400  avgSteps=  4.75  avgTotalReward= 0.57\n",
      "Done: episodes=  3500  avgSteps=  4.51  avgTotalReward= 0.60\n",
      "Done: episodes=  3600  avgSteps=  4.65  avgTotalReward= 0.57\n",
      "Done: episodes=  3700  avgSteps=  4.67  avgTotalReward= 0.59\n",
      "Done: episodes=  3800  avgSteps=  4.67  avgTotalReward= 0.57\n",
      "Done: episodes=  3900  avgSteps=  4.34  avgTotalReward= 0.62\n",
      "Done: episodes=  4000  avgSteps=  4.55  avgTotalReward= 0.60\n",
      "Done: episodes=  4100  avgSteps=  4.48  avgTotalReward= 0.60\n",
      "Done: episodes=  4200  avgSteps=  4.41  avgTotalReward= 0.62\n",
      "Done: episodes=  4300  avgSteps=  4.37  avgTotalReward= 0.63\n",
      "Done: episodes=  4400  avgSteps=  4.41  avgTotalReward= 0.62\n",
      "Done: episodes=  4500  avgSteps=  4.28  avgTotalReward= 0.64\n",
      "Done: episodes=  4600  avgSteps=  4.41  avgTotalReward= 0.62\n",
      "Done: episodes=  4700  avgSteps=  4.34  avgTotalReward= 0.62\n",
      "Done: episodes=  4800  avgSteps=  4.38  avgTotalReward= 0.63\n",
      "Done: episodes=  4900  avgSteps=  4.33  avgTotalReward= 0.63\n",
      "Done: episodes=  5000  avgSteps=  4.34  avgTotalReward= 0.63\n",
      "Done: episodes=  5100  avgSteps=  4.22  avgTotalReward= 0.63\n",
      "Done: episodes=  5200  avgSteps=  4.23  avgTotalReward= 0.64\n",
      "Done: episodes=  5300  avgSteps=  4.16  avgTotalReward= 0.65\n",
      "Done: episodes=  5400  avgSteps=  4.27  avgTotalReward= 0.64\n",
      "Done: episodes=  5500  avgSteps=  4.22  avgTotalReward= 0.66\n",
      "Done: episodes=  5600  avgSteps=  4.25  avgTotalReward= 0.64\n",
      "Done: episodes=  5700  avgSteps=  4.13  avgTotalReward= 0.65\n",
      "Done: episodes=  5800  avgSteps=  4.27  avgTotalReward= 0.65\n",
      "Done: episodes=  5900  avgSteps=  4.05  avgTotalReward= 0.67\n",
      "Done: episodes=  6000  avgSteps=  4.15  avgTotalReward= 0.65\n",
      "Done: episodes=  6100  avgSteps=  4.19  avgTotalReward= 0.65\n",
      "Done: episodes=  6200  avgSteps=  4.19  avgTotalReward= 0.65\n",
      "Done: episodes=  6300  avgSteps=  4.14  avgTotalReward= 0.68\n",
      "Done: episodes=  6400  avgSteps=  4.12  avgTotalReward= 0.68\n",
      "Done: episodes=  6500  avgSteps=  4.21  avgTotalReward= 0.65\n",
      "Done: episodes=  6600  avgSteps=  4.06  avgTotalReward= 0.69\n",
      "Done: episodes=  6700  avgSteps=  4.18  avgTotalReward= 0.67\n",
      "Done: episodes=  6800  avgSteps=  4.12  avgTotalReward= 0.68\n",
      "Done: episodes=  6900  avgSteps=  4.07  avgTotalReward= 0.70\n",
      "Done: episodes=  7000  avgSteps=  4.14  avgTotalReward= 0.69\n",
      "Done: episodes=  7100  avgSteps=  4.17  avgTotalReward= 0.67\n",
      "Done: episodes=  7200  avgSteps=  4.05  avgTotalReward= 0.69\n",
      "Done: episodes=  7300  avgSteps=  4.11  avgTotalReward= 0.69\n",
      "Done: episodes=  7400  avgSteps=  4.10  avgTotalReward= 0.70\n",
      "Done: episodes=  7500  avgSteps=  4.08  avgTotalReward= 0.70\n",
      "Done: episodes=  7600  avgSteps=  4.11  avgTotalReward= 0.69\n",
      "Done: episodes=  7700  avgSteps=  4.11  avgTotalReward= 0.70\n",
      "Done: episodes=  7800  avgSteps=  4.12  avgTotalReward= 0.70\n",
      "Done: episodes=  7900  avgSteps=  4.07  avgTotalReward= 0.70\n",
      "Done: episodes=  8000  avgSteps=  4.05  avgTotalReward= 0.71\n",
      "Done: episodes=  8100  avgSteps=  4.06  avgTotalReward= 0.71\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0012734141 |\n",
      "| clipfrac           | 0.014282227  |\n",
      "| eplenmean          | nan          |\n",
      "| eprewmean          | nan          |\n",
      "| explained_variance | 0.969        |\n",
      "| fps                | 268          |\n",
      "| nupdates           | 20           |\n",
      "| policy_entropy     | 0.2844019    |\n",
      "| policy_loss        | -0.017986292 |\n",
      "| serial_timesteps   | 40960        |\n",
      "| time_elapsed       | 164          |\n",
      "| total_timesteps    | 40960        |\n",
      "| value_loss         | 0.0012332108 |\n",
      "-------------------------------------\n",
      "Done: episodes=  8200  avgSteps=  4.09  avgTotalReward= 0.70\n",
      "Done: episodes=  8300  avgSteps=  4.07  avgTotalReward= 0.71\n",
      "Done: episodes=  8400  avgSteps=  4.03  avgTotalReward= 0.71\n",
      "Done: episodes=  8500  avgSteps=  4.07  avgTotalReward= 0.71\n",
      "Done: episodes=  8600  avgSteps=  4.04  avgTotalReward= 0.71\n",
      "Done: episodes=  8700  avgSteps=  4.04  avgTotalReward= 0.71\n",
      "Done: episodes=  8800  avgSteps=  4.02  avgTotalReward= 0.72\n",
      "Done: episodes=  8900  avgSteps=  4.04  avgTotalReward= 0.72\n",
      "Done: episodes=  9000  avgSteps=  4.04  avgTotalReward= 0.72\n",
      "Done: episodes=  9100  avgSteps=  4.06  avgTotalReward= 0.72\n",
      "Done: episodes=  9200  avgSteps=  4.07  avgTotalReward= 0.71\n",
      "Done: episodes=  9300  avgSteps=  4.04  avgTotalReward= 0.72\n",
      "Done: episodes=  9400  avgSteps=  4.00  avgTotalReward= 0.73\n",
      "Done: episodes=  9500  avgSteps=  4.04  avgTotalReward= 0.72\n",
      "Done: episodes=  9600  avgSteps=  4.05  avgTotalReward= 0.72\n",
      "Done: episodes=  9700  avgSteps=  4.03  avgTotalReward= 0.72\n",
      "Done: episodes=  9800  avgSteps=  4.08  avgTotalReward= 0.72\n",
      "Done: episodes=  9900  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 10000  avgSteps=  4.00  avgTotalReward= 0.73\n",
      "Done: episodes= 10100  avgSteps=  4.03  avgTotalReward= 0.72\n",
      "Done: episodes= 10200  avgSteps=  4.03  avgTotalReward= 0.72\n",
      "Done: episodes= 10300  avgSteps=  4.07  avgTotalReward= 0.72\n",
      "Done: episodes= 10400  avgSteps=  4.04  avgTotalReward= 0.72\n",
      "Done: episodes= 10500  avgSteps=  4.02  avgTotalReward= 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: episodes= 10600  avgSteps=  4.02  avgTotalReward= 0.73\n",
      "Done: episodes= 10700  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 10800  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 10900  avgSteps=  4.03  avgTotalReward= 0.72\n",
      "Done: episodes= 11000  avgSteps=  4.00  avgTotalReward= 0.73\n",
      "Done: episodes= 11100  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 11200  avgSteps=  4.02  avgTotalReward= 0.72\n",
      "Done: episodes= 11300  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 11400  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 11500  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 11600  avgSteps=  4.02  avgTotalReward= 0.73\n",
      "Done: episodes= 11700  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 11800  avgSteps=  4.03  avgTotalReward= 0.72\n",
      "Done: episodes= 11900  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 12000  avgSteps=  4.03  avgTotalReward= 0.73\n",
      "Done: episodes= 12100  avgSteps=  4.02  avgTotalReward= 0.73\n",
      "Done: episodes= 12200  avgSteps=  4.00  avgTotalReward= 0.73\n",
      "Done: episodes= 12300  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 12400  avgSteps=  4.00  avgTotalReward= 0.73\n",
      "Done: episodes= 12500  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 12600  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 12700  avgSteps=  4.01  avgTotalReward= 0.73\n"
     ]
    }
   ],
   "source": [
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from baselines.ppo2 import ppo2\n",
    "\n",
    "BeraterEnv.showStep = False\n",
    "BeraterEnv.showDone = False\n",
    "\n",
    "wrapped_env = DummyVecEnv([lambda: gym.make(id)])\n",
    "model = ppo2.learn(network='mlp', env=wrapped_env, total_timesteps=60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enjoy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 12771   Step:    1  S --2-> C R= 0.27 totalR= 0.27 cost= 200 customerR=1000 optimum=3000\n",
      "Episode: 12771   Step:    2  C --1-> B R= 0.25 totalR= 0.52 cost= 250 customerR=1000 optimum=3000\n",
      "Episode: 12771   Step:    3  B --0-> A R= 0.25 totalR= 0.77 cost= 250 customerR=1000 optimum=3000\n",
      "Episode: 12771   Step:    4  A --2-> S R=-0.03 totalR= 0.73 cost= 100 customerR=   0 optimum=3000\n",
      "Episode finished after 4 timesteps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "observation = wrapped_env.reset()\n",
    "state = np.zeros((1, 2*128))\n",
    "dones = np.zeros((1))\n",
    "\n",
    "BeraterEnv.showStep = True\n",
    "BeraterEnv.showDone = False\n",
    "\n",
    "for t in range(1000):\n",
    "    actions, _, state, _ = model.step(observation, S=state, M=dones)\n",
    "    observation, reward, done, info = wrapped_env.step(actions)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
