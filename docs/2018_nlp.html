<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>NLP</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

          <link rel="stylesheet" href="reveal.js/css/reveal.css">
          <!--<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">-->
          <link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
      
          <!-- Code syntax highlighting -->
          <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
          <style>
              /*pre code {*/
                  /*display: block;*/
                  /*padding: 0.5em;*/
                  /*background: #FFFFFF !important;*/
                  /*color: #000000 !important;*/
              /*}*/
      
              .right-img {
                  margin-left: 10px !important;
                  float: right;
                  height: 500px;
              }
              .todo:before {
                  content: 'TODO: ';
              }
              .todo {
                  color: red !important;
              }
              code span.line-number {
                  color: lightcoral;
              }
              .reveal pre code {
                  max-height: 1000px !important;
              }
      
              img {
                  border: 0 !important;
                  box-shadow:0 0 0 0 !important;
              }
      
              .reveal {
                  -ms-touch-action: auto !important;
                  touch-action: auto !important;
                      }
      
                      .reveal h2,
                      .reveal h3,
                      .reveal h4 {
                          letter-spacing: 2px;
                          font-family: 'Amiri', serif;
                          /* font-family: 'Times New Roman', Times, serif; */
                          font-weight: bold;
                          font-style: italic;
                          letter-spacing: -2px;
                          text-transform: none !important;
                      }
      
                      .reveal em {
                          font-weight: bold;
                      }
      
                      .reveal .step-subtitle h1 {
                          letter-spacing: 1px;
                      }
                      .reveal .step-subtitle h2,
                      .reveal .step-subtitle h3 {
                          text-transform: none;
                          font-style: italic;
                          font-weight: normal;
                          /* font-weight: 400; */
                          /* font-family: 'Amiri', serif; */
                          font-family: 'Lobster', serif;
                          letter-spacing: 1px;
                          color: #2aa198;
                          text-decoration: underline;
                      }
      
                      .reveal .front-page h1,
                      .reveal .front-page h2 {
                          font-family: "League Gothic";
                          font-style: normal;
                          text-transform: uppercase !important;
                          letter-spacing: 1px;
                      }
      
                      .reveal .front-page h1 {
                          font-size: 2.5em !important;
                      }
      
                      .reveal .highlight {
                          background-color: #D3337B;
                          color: white;
                      }
      
              .reveal section img {
                background: none;
              }
      
                      .reveal img.with-border {
                          border: 1px solid #586e75 !important;
                          box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
                      }
      
                      .reveal li {
                          margin-bottom: 8px;
                      }
      
                      /* For li's that use FontAwesome icons as bullet-point */
                  .reveal ul.fa-ul li {
                      list-style-type: none;
                  }
          </style>
      
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">
    <div class="slides">

        <section>
            <h2>Introduction to Natural Language Processing (NLP)</h2>
            <h4><a href="http://zeigermann.eu">Oliver Zeigermann</a> / <a href="http://twitter.com/djcordhose">@DJCordhose</a>
            </h4>
        </section>
        <section data-markdown class="todo">
                <textarea data-template>
- https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/                    
- LSTMs erkl√§ren nach Coah
        </textarea>
        </section>

        <section>
            <h2>Structure</h2>
            <ol>
                <li>Encoding text to make it accessible to Machine Learning
                        <ol>
                                <li>Basics</li>
                                <li>Word2Vec: Word Embeddings</li>
                            </ol>
                
                </li>
                <li>Machine Learning architectures applicable to text
                        <ol>
                                <li>RNNs</li>
                                <li>LSTMs</li>
                                <li>Applications</li>
                            </ol>
                
                </li>
            </ol>
        </section>

        <section>
            <h2>Part 1: Making text accissible to Machine Learning</h2>
            <p>Linguistic Processing</p>
        </section>

        <section data-markdown>
            <textarea data-template>
### Basic Linguistic Processing

* Tokenizing
* Stemming
* Stop-Words
* Part-of-Speach (POS) Tagging

De-facto standard Python library: https://spacy.io/
            </textarea>
        </section>
        
        <section>
                <img src="img/flashcards/Tokenizing_Text_print.png">
            </section>

            <section>
                <img src="img/flashcards/Stemming_Words_print.png">
            </section>
    
            <section>
                    <img src="img/flashcards/Stop_Words_print.png">
                </section>
        
<section data-markdown>
        <textarea data-template>
## Part-of-Speach (POS) Tagging 

https://spacy.io/usage/linguistic-features
</textarea>
</section>

<section>
        <h3>Terms</h3>
        <img src='img/nlp/spacy/pos_terms.png'>
</section>

<section data-markdown>
        <textarea data-template>
### Example 

_Apple is looking at buying U.K. startup for $1 billion_
</textarea>
</section>

<section>
    <h3>Terms for Example</h3>
    <img src='img/nlp/spacy/pos_example.png' height="500px">
    <p><small>Apple is looking at buying U.K. startup for $1 billion</small></p>
</section>

<section>
        <h3>Bringing in Knowledge about the World</h3>
        <img src='img/nlp/spacy/entity.PNG'>
    </section>

    <section>
            <h3>POS</h3>
            <img src='img/nlp/spacy/pos_example_1.png' height="250px">
            <img src='img/nlp/spacy/pos_example_2.png' height="250px">
        </section>

        <section data-markdown>
            <textarea data-template>
### How does POS work?

* There are a number of different approaches, among them hidden Markov models
* More recent approaches use Neural Networks

https://en.wikipedia.org/wiki/Part-of-speech_tagging
https://spacy.io/api/#nn-model
            </textarea>
        </section>
        
    
<section>
    <h2>Transforming Text into Vectors</h2>
</section>

<section data-markdown>
                <textarea data-template>
### Bag of Words

Classic approach to turn words into a vector representation accessible for machine learning. 

https://en.wikipedia.org/wiki/Bag-of-words_model

<small>
        http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
</small>
            </textarea>
        </section>

        <section data-markdown>
                <textarea data-template>
<img src="img/flashcards/Bag_Of_Words_print.png">
                    </textarea>
        </section>

        <section data-markdown>
                <textarea data-template>
### Limitations of Bag of Words

* Positions and context of words get lost
* No notion of semantics to words
* Common words, not specific to domain probably get highest frequencies

            </textarea>
        </section>

        <section data-markdown>
                <textarea data-template>
### TF-IDF

* approach to strip off common words from a measure
* TF: term frequency
* IDF: inverse document-frequency
* TF-IDF: TF times IDF

<div  style="font-size: x-large">
http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
</div>

<div  style="font-size: x-large">
http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html
</div>
</textarea>
        </section>


            <section>
                <img src="img/flashcards/TF-IDF_print.png">
            </section>
    
        <section>
            <h2>Word Embeddings</h2>
            <p>Taking position and context of words into account</p>
            <p>Semantics of words by proximity to other words</p>
        </section>

        <section data-markdown>
                <textarea data-template>
### Motivation: What makes text and sequences so different?
<img src="img/tf/audio-image-text.png">
https://www.tensorflow.org/tutorials/word2vec
                </textarea>
        </section>

        <section data-markdown>
            <textarea data-template>
<img src="img/nlp/word_embeddings.png" height="550px">

<small>
<a href="https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb">
Deep Learning with Python
</a>
</small>
            </textarea>
    </section>


        <section data-markdown>
                <textarea data-template>
### Distributional Hypothesis

__Words that appear in the same contexts share semantic meaning.__

https://en.wikipedia.org/wiki/Distributional_semantics
    </textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Two Categories of approaches

1. _count-based methods_: compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word
1. _predictive methods_: directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Standard Predictive Method: word2vec

Comes in two flavours

1. _Continuous Bag-of-Words model (CBOW)_: predicts target words (e.g. 'mat') from source context words ('the cat sits on the') - _good for small data sets_
1. _Skip-Gram model_: does the inverse and predicts source context-words from the target words - _good for larger datasets_. 

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
## Skip-Gram model

We will restrict ourselves to this model
</textarea>
</section>


<section data-markdown>
        <textarea data-template>
### Exmaple using the Skip-Gram model

_the quick brown fox jumped over the lazy dog_

* Form a dataset of the context of each word
* vanialla context for a word are the words left and right to it
* each context is a pair of _(context, target)_ 

_([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ..._

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Skip-Gram model inversion

* Skip-Gram tries to infer *each* context word from its target words
* each pair becomes _(input, output)_ 

_([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ..._

becomes

_(quick, the), (quick, brown), (brown, quick), (brown, fox), ..._
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### How do we train for that objective?

* objective is to learn to predict the output for the input
* that means we have to have a loss function to minimize
* minimization as in supervised learning using SGD

But how does our model look like?
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Naive approach: Expensive
<img src='img/tf/softmax-nplm.png' height="450px">

Have a probability for all words (v)
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Inexpensive Approach 

<img src='img/tf/nce-nplm.png' height="450px">

Just choose k random words
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### The loss function

* objective is maximized when the model assigns high probabilities to the real words, and low probabilities to noise words
* this is called Negative Sampling
* scales only with the number (k) of noise words that we select, not all words in the vocabulary (v)
* makes it much faster to train

This is called noise-contrastive estimation (NCE) loss
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### What are the actual parameters learned?

* vectors for Embedding per word
* in other words: where in the vector space is a word located
* similar words have similar vectors

similarity is expressed by the context a word occurs in
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### We can visualize these similarities

<img src='img/tf/linear-relationships.png'>

Using t-SNE to project from high dimensions to 2d
</textarea>
</section>

<section>
        <h2>Discussion: What kind of learning is this?</h2>
    </section>
    
    <section>
        <img src="img/sketch/types-of-ml.png">
    </section>

<section>
    <h3>Supervised vs Unsupervised</h3>
    <ol>
        <li class="fragment">there is no need manual labelling</li>
        <li class="fragment">but labelling is implicit in sequence of words in training data</li>
    </ol>
    <p class="fragment">We would still call this Unsupervised</p>
</section>

<section data-markdown>
        <textarea data-template>
### Now what?

You can either

* use the results all by themselves to find out about similarities - visualize by projecting down to 2d using t-SNE as shown before
* or use the vectors as representations for words - treat texts as sequences of words
</textarea>
</section>

<section class="todo">
        <h2>Part 2: Using Sequences of Words</h2>
    </section>


    <section>
            <h3>Text is special</h3>
            <img src='img/applications/decisions/data.png'>
        </section>


        <section>
            <h2>RNNs</h2>
            <h3>Recurrent Neural Networks</h3>
        </section>


        <section>
            <h3>Motivation</h3>
            <p>Traditional Networks have no memory of previous events</p>
        </section>

            
        <section data-markdown>
            <textarea data-template>
### Solution: RNNs - Networks with Loops
<img src='img/nlp/colah/RNN-rolled.png' height="450px">

<small>
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
</small>
            </textarea>
        </section>
            
        <section data-markdown>
            <textarea data-template>
### Unrolling the loop
<img src='img/nlp/colah/RNN-unrolled.png'>

<small>
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
</small>
            </textarea>
        </section>

        <section>
            <h2>Perfect for sequences, lists, sentences</h2>
        </section>
        
        <section data-markdown>
                <textarea data-template>
### Example: Using sequences of events
<img src='img/magenta-rnn-duck.png' height="450px">

<small>
https://twitter.com/random_forests/status/987394050914385927
</small>
                </textarea>
            </section>
        <section class="todo">
            <pre>
Durchgehen und sofort Bilder klauen
  - http://colah.github.io/posts/2015-08-Understanding-LSTMs/
  - Illustration aus anderem Artikel: https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714
- Nach RNN gleich Anwendungen in: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
- https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767

- Danach: Anwendungen f√ºr NLP wie bei Cholett

1. Stunde Intro wie hier
2. NLP wie bei Cholett
3. NLP wie Stanford Kurs

Nicht mehr planen, weil letzter Teil, der sicher zuende gebracht werden soll.
            </pre>
        </section>



        <section>
            <h2>LSTMs</h2>
        </section>

        <section data-markdown class="todo">
                <textarea data-template>
Cholet: Kap 6.2.2 als Motivation f√ºr LSTMs

SimpleRNN isn‚Äôt the only recurrent layer available in Keras. There are two others: LSTM
and GRU. In practice, you‚Äôll always use one of these, because SimpleRNN is generally too
simplistic to be of real use. SimpleRNN has a major issue: although it should theoretically
be able to retain at time t information about inputs seen many timesteps before, in
practice, such long-term dependencies are impossible to learn. This is due to the vanishing
gradient problem, an effect that is similar to what is observed with non-recurrent
networks (feedforward networks) that are many layers deep: as you keep adding layers
to a network, the network eventually becomes untrainable. The theoretical reasons for
this effect were studied by Hochreiter, Schmidhuber, and Bengio in the early 1990s.2
The LSTM and GRU layers are designed to solve this problem.
                    </textarea>
        </section>


        <section data-markdown class="todo">
                <textarea data-template>
### RNNs / LSTMs
* https://www.tensorflow.org/tutorials/recurrent
        </textarea>
        </section>
    
    <section data-markdown class="todo">
            <textarea data-template>
### Applications
* https://www.tensorflow.org/tutorials/text_classification_with_tf_hub
* https://www.tensorflow.org/tutorials/seq2seq
    </textarea>
    </section>



    <section data-markdown class="todo">
            <textarea data-template>
    # From here on just material
    </textarea>
    </section>
        



<section class="todo">
                <pre>
https://medium.com/@alyafey22/sentiment-classification-from-keras-to-the-browser-7eda0d87cdc6

Embeddings:
- https://www.quora.com/What-does-the-word-embedding-mean-in-the-context-of-Machine-Learning
- https://www.tensorflow.org/programmers_guide/embedding
- Dense Embeddings / SkipGram
  - https://www.tensorflow.org/tutorials/word2vec

TF-IDF + LogReg: 
https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py
</pre>
</section>

<section class="todo">
            <pre>

Termin 1, Intro nur 1 Stunde
* Grammars

* Intro: 1/ People (mostly people working with Computer Vision) say that CV is ahead of other ML application domains by at least 6 months - 1 year. I would like to explore why this is, if this is something to be concerned about, and what it might take to catch up.
(https://twitter.com/seb_ruder/status/980030523660791809?s=03)

* Bag Of Words, Categories of Text (SPAM Enron)
* Google Assistant
* Vielleicht: https://towardsdatascience.com/introduction-to-markov-chains-50da3645a50d
* https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa
* Der erste iX Artikel
* https://beta.observablehq.com/@jashkenas/sentiment-analysis-with-tensorflow-js

Termin 2
* Nur 3 Stunden: Nicht mehr planen, weil letzter Teil, der sicher zuende gebracht werden soll.
In der letzten Stunde zeigen wir uns sprechen √ºber die Praktikumsaufgaben, Studenten kriegen Zeit, sich
damit auseiander zu setzen

Embeddings
- https://www.tensorflow.org/tutorials/word2vec
- https://www.tensorflow.org/programmers_guide/embedding
- Our preprint that creates embeddings for over 100k medical concepts using data from 60 million patients, 1.7 million journal articles and 20 million notes is up: https://t.co/u6C7lN7jl4

Pretrained embeddings: https://t.co/bk5aStgaA3

Interactive explorer https://t.co/1MdPq40ohf https://t.co/ig8AlIasuI
(https://twitter.com/AndrewLBeam/status/981877899174318081?s=03)

1. Stunde Intro aus Colah
2. Stunde
- http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/
- https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa
- https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a
3. Stunde: NLP wie bei Cholett
* Praxis-Teil mit Notebook aus 

    * C:\Users\olive\Google Drive\ML-Buch\Docs
    * Deep_Learning_with_Python
    * Kapitel 6
    * https://github.com/fchollet/deep-learning-with-python-notebooks (ist schon gecloned)

Keras LSTMs: https://www.kdnuggets.com/2017/10/seven-steps-deep-learning-keras.html

NLP wie Stanford Kurs nur einstreuen
 * NLP with Deep Learning Course: http://web.stanford.edu/class/cs224n/

 https://machinelearningmastery.com/develop-word-embeddings-python-gensim/

            </pre>
        </section>

        <section data-markdown class="todo">
                <textarea data-template>
    <img src="img/md_dev_days/dbscan.jpg">
    
                </textarea>
    </section>

    <section data-markdown class="todo">
            <textarea data-template>
<img src="img/md_dev_days/word_encoding.jpg">

            </textarea>
</section>

    <section>
            <h3>Introduction to NLP</h3>
        </section>

        <section>
            <h3>Exercise: How to build the most basic solution</h3>
            <p>Consider you have a number of texts mapped to specific categories</p>
            <p><em>What is the most basic solution to encode the text into something that is accessible to a machine learning solution</em></p>
            <p>Disucss and groups and come up with a solution</p>
            <p><small>10 Minutes Work and 10 Minutes Discussion</small></p>
        </section>

        <section>
            <h3>Where are we in NLP today?</h3>
        </section>

        <section class="todo">
            <pre>
https://www.theverge.com/2018/3/27/17167200/google-ai-speech-tts-cloud-deepmind-wavenet
            </pre>
        </section>

        <section class="todo">
            <pre>
- Sentiment
  - http://text-processing.com/demo/sentiment/
  - http://www.nltk.org/howto/sentiment.html
  - https://www.kaggle.com/ngyptr/python-nltk-sentiment-analysis
  - https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras

- Nlp: enron dataset machine learning


            </pre>
        </section>

        <section>
            <h2>Basics of NLP</h2>
        </section>

                <section class="todo">
                    <h3>Motivation</h3>
                    <pre>
 * Fran√ßois Chollet (@fchollet) tweeted at 7:41 PM on Thu, Mar 08, 2018:It's clear to me that language (la langue, pas le
langage) has significant influence on thought -- on what one can think, and on how one thinks (https://twitter.com/fchollet/status/971818179595984896?s=03
                                    
                                </pre>
                </section>

        <section class="todo">
            <pre>
https://www.smartdatacollective.com/natural-language-processing-essential-element-ai/
                    
                </pre>
        </section>

        <section class="todo">
            <h3>Applications</h3>
            <pre>
* Chatbot
	* https://github.com/RasaHQ/rasa_nlu/blob/master/README.md
	* https://www.quora.com/How-do-I-develop-an-AI-based-chatbot-in-Python-from-scratch



            </pre>
        </section>


        <section class="todo">
<pre>
 * Spacy: https://www.oreilly.com/ideas/comparing-production-grade-nlp-libraries-accuracy-performance-and-scalability?imm_mid=0fbcdf&cmp=em-data-na-na-newsltr_ai_20180305
    
</pre>            
        </section>

    </div>

</div>

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>
<script src="lib/jquery-2.2.4.js"></script>

<script>
        if (window.location.hostname.indexOf('localhost') !== -1) {
        } else {
            // only applies to public version
            $('.todo').remove();
            $('.preparation').remove();
            $('.local').remove();
        }
        Reveal.addEventListener( 'ready', function( event ) {
            if (window.location.hostname.indexOf('localhost') !== -1) {
                // only applies to presentation version
                Reveal.configure({ controls: false });
            } else {
                // only applies to public version
                $('.fragment').removeClass('fragment');
            }
            // applies to all versions
            $('code').addClass('line-numbers');
        } );
        // $('section').attr('data-background-image', "backgrounds/light-metal.jpg");
        // $('section').attr('data-background-image', "backgrounds/pink.jpg");
        $('section').attr('data-background-image', "backgrounds/white.jpg");
    //    $('section').attr('data-background-image', "backgrounds/code.jpg");
    </script>
    
<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'fade', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
            {
                src: 'reveal.js/lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: 'reveal.js/plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: 'reveal.js/plugin/zoom-js/zoom.js', async: true},
            {src: 'reveal.js/plugin/notes/notes.js', async: true},
            // https://github.com/mikemiles86/reveal-line-numbers
            {src: 'lib/js/line-numbers.js'}
        ]
    });

</script>

</body>
</html>
