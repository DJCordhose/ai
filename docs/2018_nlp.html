<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>NLP</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

          <link rel="stylesheet" href="reveal.js/css/reveal.css">
          <!--<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">-->
          <link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
      
          <!-- Code syntax highlighting -->
          <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
          <style>
              /*pre code {*/
                  /*display: block;*/
                  /*padding: 0.5em;*/
                  /*background: #FFFFFF !important;*/
                  /*color: #000000 !important;*/
              /*}*/
      
              .right-img {
                  margin-left: 10px !important;
                  float: right;
                  height: 500px;
              }
              .todo:before {
                  content: 'TODO: ';
              }
              .todo {
                  color: red !important;
              }
              code span.line-number {
                  color: lightcoral;
              }
              .reveal pre code {
                  max-height: 1000px !important;
              }
      
              img {
                  border: 0 !important;
                  box-shadow:0 0 0 0 !important;
              }
      
              .reveal {
                  -ms-touch-action: auto !important;
                  touch-action: auto !important;
                      }
      
                      .reveal h2,
                      .reveal h3,
                      .reveal h4 {
                          letter-spacing: 2px;
                          font-family: 'Amiri', serif;
                          /* font-family: 'Times New Roman', Times, serif; */
                          font-weight: bold;
                          font-style: italic;
                          letter-spacing: -2px;
                          text-transform: none !important;
                      }
      
                      .reveal em {
                          font-weight: bold;
                      }
      
                      .reveal .step-subtitle h1 {
                          letter-spacing: 1px;
                      }
                      .reveal .step-subtitle h2,
                      .reveal .step-subtitle h3 {
                          text-transform: none;
                          font-style: italic;
                          font-weight: normal;
                          /* font-weight: 400; */
                          /* font-family: 'Amiri', serif; */
                          font-family: 'Lobster', serif;
                          letter-spacing: 1px;
                          color: #2aa198;
                          text-decoration: underline;
                      }
      
                      .reveal .front-page h1,
                      .reveal .front-page h2 {
                          font-family: "League Gothic";
                          font-style: normal;
                          text-transform: uppercase !important;
                          letter-spacing: 1px;
                      }
      
                      .reveal .front-page h1 {
                          font-size: 2.5em !important;
                      }
      
                      .reveal .highlight {
                          background-color: #D3337B;
                          color: white;
                      }
      
              .reveal section img {
                background: none;
              }
      
                      .reveal img.with-border {
                          border: 1px solid #586e75 !important;
                          box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
                      }
      
                      .reveal li {
                          margin-bottom: 8px;
                      }
      
                      /* For li's that use FontAwesome icons as bullet-point */
                  .reveal ul.fa-ul li {
                      list-style-type: none;
                  }
          </style>
      
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">
    <div class="slides">

        <section>
            <h2>Introduction to Natural Language Processing (NLP)</h2>
            <h4><a href="http://zeigermann.eu">Oliver Zeigermann</a> / <a href="http://twitter.com/djcordhose">@DJCordhose</a>
            </h4>
        </section>
        
        
        <!-- <section data-markdown class="todo">
                <textarea data-template>
        </textarea>
        </section> -->

        <section data-markdown>
                <textarea data-template>
### What is Natural Language Processing (NLP)?

Natural language processing (NLP), is the skill of a machine to understand and process human language within the context
in which it is spoken.

<small>
https://www.smartdatacollective.com/natural-language-processing-essential-element-ai/    
</small>
                </textarea>
        </section>

        <section data-markdown>
                <textarea data-template>
### Structure

1. _NLP without Machine Learning_: Basic Linguistic Processing
1. _NLP with Machine Learning_: Word Embeddings
1. _Neural Networks with Memory_: RNNs, LSTMs, GRUs
                </textarea>
        </section>

        <section data-markdown style="font-size: xx-large">
                <textarea data-template>
### Applications of NLP

* _Machine Translation_: automatic translation of one human language into another
* _Name Entity Recognition (NER)_: recognizing and identifying proper names and types (person, location or an organization) in text
* _Optical Character Recognition (OCR)_: text from the image of the printed text
* _Question-Answer Session_: determine the answer to a question asked in human language
* _Topic Segmentation_: separate text into parts being related to a specific topic
* _Speech Recognition_: convert spoken lanuage to its corresponding textual representation
* _Sentiment Analysis_: judge emotions in texts

https://www.smartdatacollective.com/natural-language-processing-essential-element-ai/    
                </textarea>
        </section>

            <section>
                <h3>Where are we in NLP today?</h3>
            </section>
            <section>
                <h4>Maschines can properly react to written language</h4>
                <img src="img/ai/gmail-understands-and-reacts-like-a-human.png" height="600px">
            </section>
            
        <section data-markdown>
                <textarea data-template>
### Wavenet: Machines can synthesize human language

<small>
Google launches more realistic text-to-speech service powered by DeepMindâ€™s AI    
</small>
<img src='img/nlp/wavenet/wavenet_progress.gif' height="250px">

<small>
Industry-leading: <audio
  src="img/nlp/wavenet/Hol_before.wav"
  controls></audio> WaveNet: <audio
  src="img/nlp/wavenet/Hol_before.wav"
  controls></audio>
<br>
<br>
https://www.theverge.com/2018/3/27/17167200/google-ai-speech-tts-cloud-deepmind-wavenet
</small>
                    </textarea>
                    </section>

        <section class="todo">
            <pre>
- Come up with more state of the art examples
            </pre>
        </section>

        <section data-markdown>
                <textarea data-template>
### Classic Application: Sentiment Analysis in Written Texts

<small>
https://en.wikipedia.org/wiki/Sentiment_analysis
</small>

Generally speaking, sentiment analysis aims to determine the attitude of a speaker, writer, or other subject with respect
to some topic or the overall contextual polarity or emotional reaction to a document, interaction, or event.

_Example: Does a Tweet mention something negative about my company?_
    
                </textarea>
            </section>

            <section>
                <h3>Cloud Natural Language: Google's ML API for Speech</h3>
                <img src="img/screenshot_sentiment_analysis.png">
                <p><small><a target="_blank" href="https://cloud.google.com/natural-language/">
                    https://cloud.google.com/natural-language/
                </a></small></p>
            </section>
                    
            <section>
                <h3>Initial Exercise: How to build sentiment analysis?</h3>
                <p>Image you have a number of texts labelled to categories of positive (1) or negative (0)</p>
                <p><em>You are free to either use machine learning or not</em></p>
                <p>Disucss in groups and come up with a solution</p>
                <p><small>10 Minutes Work and 10 Minutes Discussion</small></p>
            </section>

    
        <section class="todo">
                <pre>
Show approaches to sentiment analysis
- http://text-processing.com/demo/sentiment/
- http://www.nltk.org/howto/sentiment.html
- https://www.kaggle.com/ngyptr/python-nltk-sentiment-analysis
- https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras
    
    
                </pre>
            </section>
                
        <section>
            <h3>NLP without Machine Learning</h3>
            <p>Even when you do not use machine learning to process and analyse text, there is a couple of techniques that can be helpful</p>
            <p>These techniques can also be applied to make text accessible to machine learning</p>
        </section>

        <section data-markdown>
            <textarea data-template>
### Basic Linguistic Processing

* Tokenizing
* Stemming
* Stop-Words
* Part-of-Speach (POS) Tagging
* Bag of Words etc.

De-facto standard Python library: https://spacy.io/
Notebook: spacy-sandbox
https://spacy.io/usage/linguistic-features
</textarea>
        </section>

        
        <section>
                <img src="img/flashcards/Tokenizing_Text_print.png">
            </section>

            <section>
                <img src="img/flashcards/Stemming_Words_print.png">
            </section>
    
            <section>
                    <img src="img/flashcards/Stop_Words_print.png">
                </section>
        
<section data-markdown>
        <textarea data-template>
## Part-of-Speach (POS) Tagging 

https://spacy.io/usage/linguistic-features
</textarea>
</section>

<section>
        <h3>Terms</h3>
        <img src='img/nlp/spacy/pos_terms.png'>
</section>

<section data-markdown>
        <textarea data-template>
### Example 

_Apple is looking at buying U.K. startup for $1 billion_
</textarea>
</section>

<section>
    <h3>Terms for Example</h3>
    <img src='img/nlp/spacy/pos_example.png' height="500px">
    <p><small>Apple is looking at buying U.K. startup for $1 billion</small></p>
</section>

<section>
        <h3>Bringing in Knowledge about the World</h3>
        <img src='img/nlp/spacy/entity.PNG'>
        <p>Name Entity Recognition (NER)</p>
    </section>

    <section>
            <h3>POS</h3>
            <img src='img/nlp/spacy/pos_example_1.png' height="250px">
            <img src='img/nlp/spacy/pos_example_2.png' height="250px">
        </section>

        <section data-markdown>
            <textarea data-template>
### How does POS work?

* There are a number of different approaches, among them hidden Markov models
* More recent approaches use Neural Networks

https://en.wikipedia.org/wiki/Part-of-speech_tagging
https://spacy.io/api/#nn-model
            </textarea>
        </section>
        
<section>
    <h2>Transforming Text into Vectors</h2>
    <p>Making it accessible to Machine Learning</p>
</section>

<section data-markdown>
        <textarea data-template>
### Label Encoding    

Normalize words such that they contain only values between 0 and number_of_words_in_vocab-1.

<pre><code>text = ["paris", "paris", "tokyo", "amsterdam"]
paris = 0
tokyo = 1
amsterdam = 2
encoded_text = [0, 0, 1, 2]</code></pre>

Variation: Encode using frequency

<small>
http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html
</small>
    </textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Label Encoding    

Normalize words such that they contain only values between 0 and number_of_words_in_vocab-1.

<pre><code>text = ["paris", "paris", "tokyo", "amsterdam"]
paris = 0
tokyo = 1
amsterdam = 2
encoded_text = [0, 0, 1, 2]</code></pre>

Variation: Encode using frequency

<small>
http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html
</small>
    </textarea>
</section>

<section data-markdown>
                <textarea data-template>
### Bag of Words

Classic approach to turn words into a vector representation accessible for machine learning. 

https://en.wikipedia.org/wiki/Bag-of-words_model

<small>
http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
<br>
http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
<br>
https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/
</small>
            </textarea>
        </section>

        <section data-markdown>
                <textarea data-template>
<img src="img/flashcards/Bag_Of_Words_print.png">
                    </textarea>
        </section>

        <section data-markdown>
                <textarea data-template>
### Limitations of Bag of Words

* Positions and context of words get lost
* No notion of semantics to words
* Common words, not specific to domain probably get highest frequencies

            </textarea>
        </section>

        <section data-markdown>
                <textarea data-template>
### TF-IDF

* approach to strip off common words from a measure
* TF: term frequency
* IDF: inverse document-frequency
* TF-IDF: TF times IDF

<div  style="font-size: x-large">
http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
</div>

<div  style="font-size: x-large">
http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html
</div>
</textarea>
        </section>


            <section>
                <img src="img/flashcards/TF-IDF_print.png">
            </section>
    

            <section>
                <h2>Entering the world of NLP based on Machine Lerning</h2>
            </section>
    
        <section>
            <h3>Word Embeddings</h3>
            <p>Taking position and context of words into account</p>
            <p>Semantics of words by proximity to other words</p>
        </section>

        <section data-markdown>
                <textarea data-template>
### Motivation: What makes text and sequences so different?
<img src="img/tf/audio-image-text.png">
https://www.tensorflow.org/tutorials/word2vec
                </textarea>
        </section>

        <section data-markdown>
            <textarea data-template>
<img src="img/nlp/word_embeddings.png" height="550px">

<small>
<a href="https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb">
Deep Learning with Python
</a>
</small>
            </textarea>
    </section>


        <section data-markdown>
                <textarea data-template>
### Distributional Hypothesis

__Words that appear in the same contexts share semantic meaning.__

https://en.wikipedia.org/wiki/Distributional_semantics
    </textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Two Categories of approaches

1. _count-based methods_: compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word
1. _predictive methods_: directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Standard Predictive Method: word2vec

Comes in two flavours

1. _Continuous Bag-of-Words model (CBOW)_: predicts target words (e.g. 'mat') from source context words ('the cat sits on the') - _good for small data sets_
1. _Skip-Gram model_: does the inverse and predicts source context-words from the target words - _good for larger datasets_. 

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
## Skip-Gram model

We will restrict ourselves to this model
</textarea>
</section>


<section data-markdown>
        <textarea data-template>
### Exmaple using the Skip-Gram model

_the quick brown fox jumped over the lazy dog_

* Form a dataset of the context of each word
* vanialla context for a word are the words left and right to it
* each context is a pair of _(context, target)_ 

_([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ..._

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Skip-Gram model inversion

* Skip-Gram tries to infer *each* context word from its target words
* each pair becomes _(input, output)_ 

_([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ..._

becomes

_(quick, the), (quick, brown), (brown, quick), (brown, fox), ..._
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### How do we train for that objective?

* objective is to learn to predict the output for the input
* that means we have to have a loss function to minimize
* minimization as in supervised learning using SGD

But how does our model look like?
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Naive approach: Expensive
<img src='img/tf/softmax-nplm.png' height="450px">

Have a probability for all words (v)
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Inexpensive Approach 

<img src='img/tf/nce-nplm.png' height="450px">

Just choose k random words
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### The loss function

* objective is maximized when the model assigns high probabilities to the real words, and low probabilities to noise words
* this is called Negative Sampling
* scales only with the number (k) of noise words that we select, not all words in the vocabulary (v)
* makes it much faster to train

This is called noise-contrastive estimation (NCE) loss
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### What are the actual parameters learned?

* vectors for Embedding per word
* in other words: where in the vector space is a word located
* similar words have similar vectors

similarity is expressed by the context a word occurs in
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### We can visualize these similarities

<img src='img/tf/linear-relationships.png'>

Using t-SNE to project from high dimensions to 2d
</textarea>
</section>

<section>
        <h2>Discussion: What kind of learning is this?</h2>
    </section>
    
    <section>
        <img src="img/sketch/types-of-ml.png">
    </section>

<section>
    <h3>Supervised vs Unsupervised</h3>
    <ol>
        <li class="fragment">there is no need manual labelling</li>
        <li class="fragment">but labelling is implicit in sequence of words in training data</li>
    </ol>
    <p class="fragment">We would still call this Unsupervised</p>
</section>

<section data-markdown>
        <textarea data-template>
### Now what?

You can either

* use the results all by themselves to find out about similarities - visualize by projecting down to 2d using t-SNE as shown before
* or use the vectors as representations for words - treat texts as sequences of words
</textarea>
</section>


    </div>

</div>

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>
<script src="lib/jquery-2.2.4.js"></script>

<script>
        if (window.location.hostname.indexOf('localhost') !== -1) {
        } else {
            // only applies to public version
            $('.todo').remove();
            $('.preparation').remove();
            $('.local').remove();
        }
        Reveal.addEventListener( 'ready', function( event ) {
            if (window.location.hostname.indexOf('localhost') !== -1) {
                // only applies to presentation version
                Reveal.configure({ controls: false });
            } else {
                // only applies to public version
                $('.fragment').removeClass('fragment');
            }
            // applies to all versions
            $('code').addClass('line-numbers');
        } );
        // $('section').attr('data-background-image', "backgrounds/light-metal.jpg");
        // $('section').attr('data-background-image', "backgrounds/pink.jpg");
        $('section').attr('data-background-image', "backgrounds/white.jpg");
    //    $('section').attr('data-background-image', "backgrounds/code.jpg");
    </script>
    
<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'fade', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
            {
                src: 'reveal.js/lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: 'reveal.js/plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: 'reveal.js/plugin/zoom-js/zoom.js', async: true},
            {src: 'reveal.js/plugin/notes/notes.js', async: true},
            // https://github.com/mikemiles86/reveal-line-numbers
            {src: 'lib/js/line-numbers.js'}
        ]
    });

</script>

</body>
</html>
