<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>DL Workshop</title>

    <meta name="description" content="Manning Course Material">
    <meta name="author" content="Oliver Zeigermann">

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

          <link rel="stylesheet" href="reveal.js/css/reveal.css">
          <!--<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">-->
          <link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
      
          <!-- Code syntax highlighting -->
          <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
          <style>
              /*pre code {*/
                  /*display: block;*/
                  /*padding: 0.5em;*/
                  /*background: #FFFFFF !important;*/
                  /*color: #000000 !important;*/
              /*}*/
      
              .right-img {
                  margin-left: 10px !important;
                  float: right;
                  height: 500px;
              }
              .todo:before {
                  content: 'TODO: ';
              }
              .todo {
                  color: red !important;
              }
              .beginning:before {
                  content: 'BEGINNING';
              }
              .beginning {
                color: red !important;
              }
              .end:before {
                  content: 'END';
              }
              .end {
                color: red !important;
              }
              code span.line-number {
                  color: lightcoral;
              }
              .reveal pre code {
                  max-height: 1000px !important;
              }
      
              img {
                  border: 0 !important;
                  box-shadow:0 0 0 0 !important;
              }
      
              .reveal {
                  -ms-touch-action: auto !important;
                  touch-action: auto !important;
                      }
      
                      .reveal h2,
                      .reveal h3,
                      .reveal h4 {
                          letter-spacing: 2px;
                          font-family: 'Calibri', sans-serif;
                          /* font-family: 'Times New Roman', Times, serif; */
                          font-weight: bold;
                          color: black;
                          font-style: italic;
                          letter-spacing: -2px;
                          text-transform: none !important;
                      }
      
                      .reveal em {
                          font-weight: bold;
                      }
      
                      .reveal .step-subtitle h1 {
                          letter-spacing: 1px;
                      }
                      .reveal .step-subtitle h2,
                      .reveal .step-subtitle h3 {
                          text-transform: none;
                          font-style: italic;
                          font-weight: normal;
                          /* font-weight: 400; */
                          /* font-family: 'Amiri', serif; */
                          font-family: 'Lobster', serif;
                          letter-spacing: 1px;
                          color: #2aa198;
                          text-decoration: underline;
                      }
      
                      .reveal .front-page h1,
                      .reveal .front-page h2 {
                          font-family: "League Gothic";
                          font-style: normal;
                          text-transform: uppercase !important;
                          letter-spacing: 1px;
                      }
      
                      .reveal .front-page h1 {
                          font-size: 2.5em !important;
                      }
      
                      .reveal .highlight {
                          background-color: #D3337B;
                          color: white;
                      }
      
              .reveal section img {
                background: none;
              }
      
                      .reveal img.with-border {
                          border: 1px solid #586e75 !important;
                          box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
                      }
      
                      .reveal li {
                          margin-bottom: 8px;
                      }
      
                      /* For li's that use FontAwesome icons as bullet-point */
                  .reveal ul.fa-ul li {
                      list-style-type: none;
                  }
          </style>
      
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        var printMode = window.location.search.match(/print-pdf/gi);
        link.href = printMode ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>


<div class="reveal">
    <div class="slides">

<!-- 
Man erzählt sich tolle Dinge von Deep Learning und auch TensorFlow. Die spannendsten Neuerungen basieren auf diesem
Ansatz und dieser Software.

Dazu passend lernst du im ersten Teil dieses Workshops wie Neuronale Netzwerke funktionieren und was du mit ihnen
machen kannst. Wir werden dabei eigene Netzwerke für ein Klassifikationsproblem aufbauen und trainieren.

Im zweiten Teil beschäftigen wir uns mit den besonderen Netzwerkarten für Bilderkennung (CNNs) und
Textklassifikation/Sequenzen (RNNs). Dieser Teil wird durch Notebooks mit TensorFlow und Keras Code unterstützt.

Vorausgesetzt wird grundlegendes Wissen über Machine Learning wie du es z.B. beim Workshop ‚Machine Learning auch für
Dein Projekt‘ erwerben kannst. Du musst nichts auf deinem Rechner installieren, es reicht ein Laptop mit einem
aktuellen Browser, am besten Chrome.
 -->

<section data-markdown class="preparation">
        <textarea data-template>
### Preparation

    </textarea>
</section>

<section>
    <h2>Einführung in Deep Learning</h2>
    <h3> mit TensorFlow und Keras, NNs, CNNs, RNNs, LSTMS/GRUs</h3>
    <p><a target="_blank" href="https://ml-summit.de/machine-learning-basics-and-tools/einfuehrung-in-deep-learning-mit-tensorflow-und-keras-nns-cnns-rnns-lstmsgrus/">
        ML Summit, Berlin, October 2018
    </a></p>
    <h4><a href="http://zeigermann.eu">Oliver Zeigermann</a> / 
        <a href="http://twitter.com/djcordhose">@DJCordhose</a> /
        <a href="https://www.embarc.de/ ">embarc GmbH</a>
    </h4>
    <small>
    <a href="https://djcordhose.github.io/ai/2018_ml_summit_deep_learning_workshop.html">
        https://djcordhose.github.io/ai/2018_ml_summit_deep_learning_workshop.html</a>
    </small>
</section>

<section data-markdown>
        <textarea data-template>

<img src='img/twitter-fchollet-trend.png' height="500px">            

<small>
https://twitter.com/fchollet/status/1029477656876613632
<br>
https://trends.google.com/trends/explore?cat=1299&date=today%205-y&q=tensorflow,keras,pytorch,caffe,theano
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Agenda


_14:00 Uhr: Mittagspause_

* Training eines Neuronalen Netzwerks mit TensorFlow und Keras

_15.30 - 15.45 Uhr: Kaffeepause_

* Neuronale Netzwerke (CNNs) für Bildverarbeitung
* Recurrente Neuronale Netzwerke (RNNs) für Sequenzen und Textverarbeitung

_ca. 17.30 Uhr: Ende_
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### PART I
## Deep Neural Networks using TensorFlow and Keras
        </textarea>
    </section>

    <section data-markdown>
        <textarea data-template>
### TensorFlow and Keras

* https://www.tensorflow.org 
* https://www.tensorflow.org/guide/low_level_intro 
* https://www.tensorflow.org/guide/keras 

        </textarea>
    </section>

    <section>
        <h3>Example: Customer Data - Risk of Accidents</h3>
        <img src="img/manning/all.png" height="400px" class="fragment">
        <p class="fragment">
            <small>How would you rank me (47) for a car having 100 mph top speed, driving 10k miles per year?</small>
        </p>
    </section>

<section data-markdown>
            <textarea data-template>
### Working with Colab Notebooks

https://colab.research.google.com
            </textarea>
        </section>
    
<section data-markdown>
    <textarea data-template>
### Hands-On
_Run your first Colab Notbook_

* Go to https://colab.research.google.com 
* Sign into your Google account or register a new one 
* Switch on GPU support
* Execute some code cells

https://colab.research.google.com

</textarea>
</section>
    
    <section data-markdown>
        <textarea data-template>
## How does an artificial neuron work?

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
<img src='img/scans/neuron21.jpg'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
<img src='img/scans/neuron211.jpg'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
<img src='img/scans/neuron212.jpg'>

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Step

<img src="img/cnn/step.png" height="450">

<small>switching from zero to one,
    original version simulating transition from passive to active</small>
</textarea>
    </section>

            <section data-markdown>
        <textarea data-template>
### Sigmoid

<img src="img/cnn/sigmoid.png" height="450">

<small>compressing between 0 and 1,
        continuously differentiable version of step function</small>
</textarea>
    </section>

<section data-markdown>
        <textarea data-template>
### Hyperbolic Tangent

<img src="img/tanh-activation.png" height="450">

<small>floating from -1 to 1,
        like Sigmoid, but can also be negative</small>
</textarea>
    </section>

<section data-markdown>
        <textarea data-template>
### Relu: Rectified Linear Unit

<img src="img/cnn/relu.png" height="450">

<small>negative values zeroed out, positive linear,
        fights vanishing gradient</small>
</textarea>
    </section>

<section data-markdown>
        <textarea data-template>
### Complete Example

<img src='img/scans/neuron213.jpg'>

</textarea>
</section>



<section data-markdown>
        <textarea data-template>
## Setting up a complete network from neurons
        </textarea>
    </section>
 
 
<section data-markdown>
        <textarea data-template>
### What goes in?

<img src='img/scans/data_encoding.jpg'>

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### What comes out?

<img src='img/scans/encoding2.jpg'>

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Role of the Hidden Layer(s)

<img src='img/scans/encoding3.jpg'>

</textarea>
</section>

<section>
    <h3>Keras Layers</h3>

    <p><small>Sequential Model</small></p>
    <pre><code contenteditable data-trim class="fragment line-numbers javascript">
model = keras.Sequential()
        </code></pre>

    <p><small>Fully Connected Hidden Layer</small></p>
    <pre><code contenteditable data-trim class="fragment line-numbers javascript">
model.add(Dense(units=50, input_dim=3))
</code></pre>

        <p><small>Softmax Output Layer</small></p>
        <pre><code contenteditable data-trim class="fragment line-numbers javascript">
model.add(Dense(units=3, activation='softmax'))
        </code></pre>
                            
    <small>
            <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers">
                https://www.tensorflow.org/api_docs/python/tf/keras/layers
            </a>
    </small>
</p>
</section>

<section>
    <h3>How does learning work?</h3>
    <p class="fragment">This boils down to an optimization problem</p>
    <p class="fragment">The loss to be minimized is calculated from the difference between the softmax output and the known true category</p>
            <pre><code contenteditable data-trim class="fragment line-numbers python">
model.compile(loss='sparse_categorical_crossentropy',
             optimizer='adam',
             metrics=['accuracy'])
            </code></pre>
</section>

<section>
<h3>What does the neural network learn?</h3>
<p class="fragment">All the weights of a the neurons</p>
<pre><code contenteditable data-trim class="fragment line-numbers python">
model.summary()</code></pre>
<pre><code contenteditable data-trim class="fragment line-numbers python">
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
hidden1 (Dense)              (None, 50)                200       
_________________________________________________________________
softmax (Dense)              (None, 3)                 153       
=================================================================
Total params: 353
Trainable params: 353
Non-trainable params: 0
_________________________________________________________________</code></pre>
</section>


<section>
        <h3>Exercise</h3>
        <p><em>Can you explain the number of parameters for each layer for the model described in the previous slide?</em></p>
    
</section>

    
<section data-markdown>
        <textarea data-template>
### Generalization

_We do not have any idea how well our model performs, yet_

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Evaluating our model

* The most important property of a model is if it generalizes well to unknown data
* A machine learning model is of no use if it only works well on the data it has been trained on
  * If it was, the easiest way to achieve this would be a dictionary translating from a set of inputs to the known output
* Conceptually it is a little bit hard to optimize for something you do not know
* So, we introduce a little trick here

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Split known data into training and test

<img class='fragment' src='img/scans/generalization.jpg'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Use some training data for validation

<img class='fragment' src='img/scans/generalization1.jpg'>

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
## Notebook            
### Train the neural network

<small>
https://colab.research.google.com/github/djcordhose/ai/blob/master/notebooks/tensorflow/nn-training.ipynb
</small>
    </textarea>
    </section>


<section data-markdown>
        <textarea data-template>
## Exercise

_Train the model_

* Run the notebook as is
* Try to improve the model
* How well does it perform?
* Any idea why it performs the way it does?

Best known models can reach up to 80% of accuracy
    </textarea>
    </section>

    <section data-markdown>
        <textarea data-template>
### Best known model using 2 dimensions

<img src='img/manning/nn-reg.png' height="500">

<p><small>up to 73% predictions correct on previously unknown data possible</small></p>
</textarea>
    </section>

    <section data-markdown>
        <textarea data-template>
### PART II
## Other types of neural networks
        </textarea>
    </section>


    <section>
            <img src='img/applications/decisions/data.png'>
    </section>

    <section data-markdown>
            <textarea data-template>
    ## CNN - Convolutional neural networks
    ### Let the GPU burn
            </textarea>
        </section>
    
    
        <section>
            <h3>Use of GPU for non symbolic data</h3>
            <img src="albon-gpu-gaming.png">
            <p>
                <small>
                    <a href="https://twitter.com/chrisalbon/status/907028933693947904?s=03" target="_blank">
                        https://twitter.com/chrisalbon/status/907028933693947904?s=03</a>
                </small>
            </p>
        </section>
        
        <section>
            <h3>Why the recent break throughs?</h3>
            <div class="fragment" style="float: left">
                <img src="img/cray2.png" height="250">
                <p>
                    <small>Cray X-MP
                        <br> Supercomputer (1982)</small>
                </p>
            </div>
            <div class="fragment" style="float: left; padding-left: 20px; padding-top: 120px; font-weight: bold">
                x 100.000 =
            </div>
            <div class="fragment" style="float: left">
                <img src="img/titan5.jpg" height="250" style="float: right">
                <p>
                    <small>
                        <br>Titan 5 im Gamer PC (2017)</small>
                </p>
            </div>
        </section>

                <section>
                    <h3>... but we also have</h3>
                    <ol>
                        <li>Smarter Learning Strategies (more hidden layers = Deep Learning, Convolutional Layers)
                        <li>Big Data
                    </ol>
                </section>


        <section>
            <h3>GPUs work in parallel</h3>
            <div class="fragment" style="float: left; padding-left: 100px">
                <img src="img/sequential-knive.jpg" height="400">
                <p><small><em>sequential</em>, <br>slow but flexible</small>
                </p>
            </div>
            <div class="fragment" style="float: right; padding-right: 100px">
                <img src="img/parallel-knive.jpg" height="400">
                <p><small><em>parallel</em>, <br>fast but same operation for all data
                        </small>
                </p>
            </div>
        </section>


<section>
    <h3>Architectures of Convolutional Neural Networks: VGG</h3>
        <img src="img/sketch/vgg.png" height="350px">
        <p>
            <small>There are a number of specialized neural network layers</small>
        </p>
</section>


<section data-markdown>
    <textarea data-template>
### Convolutional Blocks: Cascading many Convolutional Layers having down sampling in between

![Applying filters](http://cs231n.github.io/assets/cnn/cnn.jpeg)

http://cs231n.github.io/convolutional-networks/#conv
</textarea>
</section>

<section data-markdown style="font-size: x-large">
    <textarea data-template>
### Example of a Convolution
![Dog](https://github.com/DJCordhose/speed-limit-signs/raw/master/img/conv/dog.png)
#### Many convolutional filters applied over all channels
![Dog after Convolutional Filters applied](https://github.com/DJCordhose/speed-limit-signs/raw/master/img/conv/dog-conv1.png)
http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html
</textarea>
</section>


<section data-markdown>
    <textarea data-template>
### Downlsampling Layer: Reduces data sizes and risk of overfitting
![Pooling](http://cs231n.github.io/assets/cnn/pool.jpeg)
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Max Pooling
![Max Pooling](http://cs231n.github.io/assets/cnn/maxpool.jpeg)
http://cs231n.github.io/convolutional-networks/#pool
</textarea>
</section>


<section data-markdown>
    <textarea data-template>
### Standard CNN Architecture

![Performance of CNN Architectures](https://cdn-images-1.medium.com/max/1600/1*kBpEOy4fzLiFxRLjpxAX6A.png)

<small>
https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
    ### Typical Architecture of a CNN 
![VGG architecture](img/sketch/vgg.png)

_The classifier more or less is what we used for our previous example_    
    </textarea>
    </section>

    <section>
            <h3>MNIST - Using a model <em>already trained</em></h3>
            <p>Exploring the different types layers together</p>
            <a href="https://transcranial.github.io/keras-js/#/mnist-cnn" target="_blank">
                <img src="img/browser/keras-browser.png" height="350px">
            </a>
            <p><small>
                <a href="https://transcranial.github.io/keras-js/#/mnist-cnn" target="_blank">https://transcranial.github.io/keras-js/#/mnist-cnn</a>
            </small></p>
        </section>

    <section>
            <h3>Keras layers</h3>

            <p><small>Convolution</small></p>
            <pre><code contenteditable data-trim class="fragment line-numbers javascript">
    model.add(Conv2D(filters=32, padding='same', activation='relu'))
                </code></pre>

                <p><small>Max Pooling</small></p>
                <pre><code contenteditable data-trim class="fragment line-numbers javascript">
model.add(MaxPooling2D())
                </code></pre>
                                    
                <p><small>Flatten 2d to make it accessible to Dense layers</small></p>
            <pre><code contenteditable data-trim class="fragment line-numbers javascript">
model.add(Flatten())
            </code></pre>
        <p>
            <small>
                    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers">
                        https://www.tensorflow.org/api_docs/python/tf/keras/layers
                    </a>
            </small>
        </p>
    </section>
    
    
        
        <section>
            <h3>More complex architecture: Google Inception V3</h3>
            <img src="img/inception_v3_architecture.png" height="400px">
            <p>
                <small>
                    Paper: <a href="https://arxiv.org/abs/1409.4842" target="_blank">Going Deeper with Convolutions</a>
                    <br>
                    <a href="https://stackoverflow.com/questions/39352108/does-the-inception-model-have-two-softmax-outputs" target="_blank">
                    Why two classifiers?</a>
                </small>
            </p>
        </section>

                </section>
        <section data-markdown>
                <textarea data-template>
### Fashion MNINST example

28x28 grayscale images of fashion Items

<img src="img/fashion-mnist-sprite.png" height="300px">

<small>
Tutorial: https://medium.com/tensorflow/hello-deep-learning-fashion-mnist-with-keras-50fcff8cd74a
<br><br>
https://colab.research.google.com/github/djcordhose/ai/blob/master/notebooks/tensorflow/nn-training.ipynb
</small>
        </textarea>
        </section>
        
        <section data-markdown>
                <textarea data-template>
### Exercise

_Can you improve the model for Fashion MNINST notebook?_

* other/more/less layers
* different sequence, less/more filters
* prevent overfitting even better 
</textarea>
</section>


    <section data-markdown>
        <textarea data-template>
## RNNs
### Recurrent Neural Networks
        </textarea>
    </section>

<section data-markdown>
        <textarea data-template>
### Classic Application: Sentiment Analysis in Written Texts

<small>
https://en.wikipedia.org/wiki/Sentiment_analysis
</small>

Generally speaking, sentiment analysis aims to determine the attitude of a speaker, writer, or other subject with respect
to some topic or the overall contextual polarity or emotional reaction to a document, interaction, or event.

_Example: Does a Tweet mention something negative about my company?_

        </textarea>
    </section>

            <section>
                <h3>Cloud Natural Language: Google's ML API for Speech</h3>
                <img src="img/screenshot_sentiment_analysis.png">
                <p><small><a target="_blank" href="https://cloud.google.com/natural-language/">
                    https://cloud.google.com/natural-language/
                </a></small></p>
            </section>

            <section data-markdown>
                    <textarea data-template>
## First Challenge

### How to turn words into numbers?
                        </textarea>
                        </section>

<section data-markdown>
        <textarea data-template>
### Word Embeddings

<a href='https://projector.tensorflow.org'>
<img src="img/nlp/embedding-projector.png" height="500px">
</a>

<small>
https://projector.tensorflow.org
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Exercise: Get an intuition for Word Embeddings

* Switch to T-SNE projections
* Zoom into a cluster
* Have a look at the words in the cluster
* Are they semantically related?

<small>
https://projector.tensorflow.org
</small>

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
## Second Challenge

### Traditional Networks have no memory of previous events

Which is required to "understand" text
            </textarea>
            </section>
        
    <section data-markdown>
        <textarea data-template>
### Solution: RNNs - Networks with Loops
<img src='img/nlp/colah/RNN-rolled.png' height="450px">

<small>
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
</small>
        </textarea>
    </section>
        
    <section data-markdown>
        <textarea data-template>
### Unrolling the loop
<img src='img/nlp/colah/RNN-unrolled.png'>

<small>
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
</small>
        </textarea>
    </section>

<section data-markdown>
        <textarea data-template>
## Creating our very own sentiment analyzis
### Using embeddings to train recurrent networks

<small>
https://colab.research.google.com/github/djcordhose/ai/blob/master/notebooks/tensorflow/sentiment-gru.ipynb
</small>
   
        </textarea>
        </section>

<section data-markdown>
        <textarea data-template>
### Exercise

* Tweak number of neurons
* Advanced: Make the RNN bidirectional

Can you improve on the results?

https://keras.io/layers/wrappers/#bidirectional

    </textarea>
</section>


<section data-markdown>
        <textarea data-template>
## What's next?

### Finding data sets to play with
    
* Google released a search engine for datasets
  * Search: https://toolbox.google.com/datasetsearch
  * Launch blog post: https://www.blog.google/products/search/making-it-easier-discover-datasets/
* Kaggle Datasets: https://www.kaggle.com/datasets    
    </textarea>
    </section>
    </div>

</div>

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>
<script src="lib/jquery-2.2.4.js"></script>

<script>
        if (window.location.hostname.indexOf('localhost') !== -1 && !printMode) {
        } else {
            // only applies to public version
            $('.todo').remove();
            $('.preparation').remove();
            $('.local').remove();
        }
        Reveal.addEventListener( 'ready', function( event ) {
            // do we want this???
            $('li').addClass('fragment')

            if (window.location.hostname.indexOf('localhost') !== -1) {
                // only applies to presentation version
                Reveal.configure({ controls: false });
            } else {
                // only applies to public version
                $('.fragment').removeClass('fragment');
            }
            // applies to all versions
            $('code').addClass('line-numbers');

            // make all links open in new tab
            $('a').attr('target', '_blank')

        } );
        // $('section').attr('data-background-image', "backgrounds/light-metal.jpg");
        // $('section').attr('data-background-image', "backgrounds/pink.jpg");
        $('section').attr('data-background-image', "backgrounds/white.jpg");
        // $('section').attr('data-background-image', "backgrounds/murmel2.jpg");
        // $('section').attr('data-background', "img/manning/background/m0.jpg");
        // $('section').attr('data-background', "img/manning/background/m1.jpg");
        // $('section:not([data-background])').attr('data-background', "img/manning/background/m1.jpg");
        // $('section').attr('data-background-size', "1620px");

    //    $('section').attr('data-background-image', "backgrounds/code.jpg");
    </script>
    
<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: false,
        history: true,
        center: true,
        width: 1100,

        transition: 'fade', // none/fade/slide/convex/concave/zoom

        math: {
            mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

        // Optional reveal.js plugins
        dependencies: [
            {
                src: 'reveal.js/lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: 'reveal.js/plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: 'reveal.js/plugin/zoom-js/zoom.js', async: true},
            {src: 'reveal.js/plugin/notes/notes.js', async: true},
            // https://github.com/mikemiles86/reveal-line-numbers
            {src: 'lib/js/line-numbers.js'},
            { src: 'reveal.js/plugin/math/math.js', async: true }
        ]
    });

</script>

</body>
</html>
