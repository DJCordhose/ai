<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>DL Workshop</title>

    <meta name="description" content="Manning Course Material">
    <meta name="author" content="Oliver Zeigermann">

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

          <link rel="stylesheet" href="reveal.js/css/reveal.css">
          <!--<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">-->
          <link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
      
          <!-- Code syntax highlighting -->
          <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
          <style>
              /*pre code {*/
                  /*display: block;*/
                  /*padding: 0.5em;*/
                  /*background: #FFFFFF !important;*/
                  /*color: #000000 !important;*/
              /*}*/
      
              .right-img {
                  margin-left: 10px !important;
                  float: right;
                  height: 500px;
              }
              .todo:before {
                  content: 'TODO: ';
              }
              .todo {
                  color: red !important;
              }
              .beginning:before {
                  content: 'BEGINNING';
              }
              .beginning {
                color: red !important;
              }
              .end:before {
                  content: 'END';
              }
              .end {
                color: red !important;
              }
              code span.line-number {
                  color: lightcoral;
              }
              .reveal pre code {
                  max-height: 1000px !important;
              }
      
              img {
                  border: 0 !important;
                  box-shadow:0 0 0 0 !important;
              }
      
              .reveal {
                  -ms-touch-action: auto !important;
                  touch-action: auto !important;
                      }
      
                      .reveal h2,
                      .reveal h3,
                      .reveal h4 {
                          letter-spacing: 2px;
                          font-family: 'Calibri', sans-serif;
                          /* font-family: 'Times New Roman', Times, serif; */
                          font-weight: bold;
                          color: black;
                          font-style: italic;
                          letter-spacing: -2px;
                          text-transform: none !important;
                      }
      
                      .reveal em {
                          font-weight: bold;
                      }
      
                      .reveal .step-subtitle h1 {
                          letter-spacing: 1px;
                      }
                      .reveal .step-subtitle h2,
                      .reveal .step-subtitle h3 {
                          text-transform: none;
                          font-style: italic;
                          font-weight: normal;
                          /* font-weight: 400; */
                          /* font-family: 'Amiri', serif; */
                          font-family: 'Lobster', serif;
                          letter-spacing: 1px;
                          color: #2aa198;
                          text-decoration: underline;
                      }
      
                      .reveal .front-page h1,
                      .reveal .front-page h2 {
                          font-family: "League Gothic";
                          font-style: normal;
                          text-transform: uppercase !important;
                          letter-spacing: 1px;
                      }
      
                      .reveal .front-page h1 {
                          font-size: 2.5em !important;
                      }
      
                      .reveal .highlight {
                          background-color: #D3337B;
                          color: white;
                      }
      
              .reveal section img {
                background: none;
              }
      
                      .reveal img.with-border {
                          border: 1px solid #586e75 !important;
                          box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
                      }
      
                      .reveal li {
                          margin-bottom: 8px;
                      }
      
                      /* For li's that use FontAwesome icons as bullet-point */
                  .reveal ul.fa-ul li {
                      list-style-type: none;
                  }
          </style>
      
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        var printMode = window.location.search.match(/print-pdf/gi);
        link.href = printMode ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>


<div class="reveal">
    <div class="slides">

<!-- 
Man erzählt sich tolle Dinge von Deep Learning und auch TensorFlow. Die spannendsten Neuerungen basieren auf diesem
Ansatz und dieser Software.

Dazu passend lernst du im ersten Teil dieses Workshops wie Neuronale Netzwerke funktionieren und was du mit ihnen
machen kannst. Wir werden dabei eigene Netzwerke für ein Klassifikationsproblem aufbauen und trainieren.

Im zweiten Teil beschäftigen wir uns mit den besonderen Netzwerkarten für Bilderkennung (CNNs) und
Textklassifikation/Sequenzen (RNNs). Dieser Teil wird durch Notebooks mit TensorFlow und Keras Code unterstützt.

Vorausgesetzt wird grundlegendes Wissen über Machine Learning wie du es z.B. beim Workshop ‚Machine Learning auch für
Dein Projekt‘ erwerben kannst. Du musst nichts auf deinem Rechner installieren, es reicht ein Laptop mit einem
aktuellen Browser, am besten Chrome.
 -->

<section data-markdown class="preparation">
        <textarea data-template>
### Preparation

    </textarea>
</section>

<section>
    <h2>Einführung in Deep Learning</h2>
    <h3> mit TensorFlow und Keras, NNs, CNNs, RNNs, LSTMS/GRUs</h3>
    <p><a target="_blank" href="https://ml-summit.de/machine-learning-basics-and-tools/einfuehrung-in-deep-learning-mit-tensorflow-und-keras-nns-cnns-rnns-lstmsgrus/">
        ML Summit, Berlin, October 2018
    </a></p>
    <h4><a href="http://zeigermann.eu">Oliver Zeigermann</a> / 
        <a href="http://twitter.com/djcordhose">@DJCordhose</a> /
        <a href="https://www.embarc.de/ ">embarc GmbH</a>
    </h4>
    <small>
    <a href="https://djcordhose.github.io/ai/2018_ml_summit_deep_learning_workshop.html">
        https://djcordhose.github.io/ai/2018_ml_summit_deep_learning_workshop.html</a>
    </small>
</section>

<section data-markdown>
        <textarea data-template>

<img src='img/twitter-fchollet-trend.png' height="500px">            

<small>
https://twitter.com/fchollet/status/1029477656876613632
<br>
https://trends.google.com/trends/explore?cat=1299&date=today%205-y&q=tensorflow,keras,pytorch,caffe,theano
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Agenda


_14:00 Uhr: Mittagspause_

* Training eines Neuronalen Netzwerks mit TensorFlow und Keras

_15.30 - 15.45 Uhr: Kaffeepause_

* Neuronale Netzwerke (CNNs) für Bildverarbeitung
* Recurrente Neuronale Netzwerke (RNNs) für Sequenzen und Textverarbeitung

_ca. 17.30 Uhr: Ende_
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### PART I
## Deep Neural Networks using TensorFlow and Keras
        </textarea>
    </section>

    <section data-markdown>
        <textarea data-template>
### TensorFlow and Keras

* https://www.tensorflow.org 
* https://www.tensorflow.org/guide/low_level_intro 
* https://www.tensorflow.org/guide/keras 

        </textarea>
    </section>

    <section>
        <h3>Example: Customer Data - Risk of Accidents</h3>
        <img src="img/manning/all.png" height="400px" class="fragment">
        <p class="fragment">
            <small>How would you rank me (47) for a car having 100 mph top speed, driving 10k miles per year?</small>
        </p>
    </section>

<section data-markdown>
            <textarea data-template>
### Working with Colab Notebooks

https://colab.research.google.com
            </textarea>
        </section>
    
<section data-markdown>
    <textarea data-template>
### Hands-On
_Run your first Colab Notbook_

* Go to https://colab.research.google.com 
* Sign into your Google account or register a new one 
* Switch on GPU support
* Execute some code cells

https://colab.research.google.com

</textarea>
</section>
    
    <section data-markdown>
        <textarea data-template>
## How does an artificial neuron work?

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
<img src='img/scans/neuron21.jpg'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
<img src='img/scans/neuron211.jpg'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
<img src='img/scans/neuron212.jpg'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
<img src='img/scans/neuron213.jpg'>

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
## Setting up a complete network from neurons
        </textarea>
    </section>
 
 
<section data-markdown>
        <textarea data-template>
### What goes in?

<img src='img/scans/data_encoding.jpg'>

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### What comes out?

<img src='img/scans/encoding2.jpg'>

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Role of the Hidden Layer(s)

<img src='img/scans/encoding3.jpg'>

</textarea>
</section>

<section>
    <h3>Keras Layers</h3>

    <p><small>Sequential Model</small></p>
    <pre><code contenteditable data-trim class="fragment line-numbers javascript">
model = keras.Sequential()
        </code></pre>

    <p><small>Fully Connected Hidden Layer</small></p>
    <pre><code contenteditable data-trim class="fragment line-numbers javascript">
model.add(Dense(units=50, input_dim=3))
</code></pre>

        <p><small>Softmax Output Layer</small></p>
        <pre><code contenteditable data-trim class="fragment line-numbers javascript">
model.add(Dense(units=3, activation='softmax'))
        </code></pre>
                            
    <small>
            <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers">
                https://www.tensorflow.org/api_docs/python/tf/keras/layers
            </a>
    </small>
</p>
</section>

<section>
    <h3>How does learning work?</h3>
    <p class="fragment">This boils down to an optimization problem</p>
    <p class="fragment">The loss to be minimized is calculated from the difference between the softmax output and the known true category</p>
            <pre><code contenteditable data-trim class="fragment line-numbers python">
model.compile(loss='sparse_categorical_crossentropy',
             optimizer='adam',
             metrics=['accuracy'])
            </code></pre>
</section>

<section>
<h3>What does the neural network learn?</h3>
<p class="fragment">All the weights of a the neurons</p>
<pre><code contenteditable data-trim class="fragment line-numbers python">
model.summary()</code></pre>
<pre><code contenteditable data-trim class="fragment line-numbers python">
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
hidden1 (Dense)              (None, 50)                200       
_________________________________________________________________
softmax (Dense)              (None, 3)                 153       
=================================================================
Total params: 353
Trainable params: 353
Non-trainable params: 0
_________________________________________________________________</code></pre>
</section>


<section>
        <h3>Exercise</h3>
        <p><em>Can you explain the number of parameters for each layer for the model described in the previous slide?</em></p>
    
</section>

    
<section data-markdown>
        <textarea data-template>
### Generalization

_We do not have any idea how well our model performs, yet_

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Evaluating our model

* The most important property of a model is if it generalizes well to unknown data
* A machine learning model is of no use if it only works well on the data it has been trained on
  * If it was, the easiest way to achieve this would be a dictionary translating from a set of inputs to the known output
* Conceptually it is a little bit hard to optimize for something you do not know
* So, we introduce a little trick here

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Split known data into training and test

<img class='fragment' src='img/scans/generalization.jpg'>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Use some training data for validation

<img class='fragment' src='img/scans/generalization1.jpg'>

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
## Notebook            
### Train the neural network

<small>
https://colab.research.google.com/github/djcordhose/ai/blob/master/notebooks/tensorflow/nn-training.ipynb
</small>
    </textarea>
    </section>


<section data-markdown>
        <textarea data-template>
## Exercise

_Train the model_

* Run the notebook as is
* Try to improve the model
* How well does it perform?
* Any idea why it performs the way it does?

Best known models can reach up to 80% of accuracy
    </textarea>
    </section>

    <section data-markdown>
        <textarea data-template>
### Best known model using 2 dimensions

<img src='img/manning/nn-reg.png' height="500">

<p><small>up to 73% predictions correct on previously unknown data possible</small></p>
</textarea>
    </section>

    <section data-markdown>
        <textarea data-template>
### PART II
## Other types of neural networks
        </textarea>
    </section>


    <section>
            <img src='img/applications/decisions/data.png'>
    </section>

        <section>
            <h3>GPUs work in parallel</h3>
            <div class="fragment" style="float: left; padding-left: 100px">
                <img src="img/sequential-knive.jpg" height="400">
                <p><small><em>sequential</em>, <br>slow but flexible</small>
                </p>
            </div>
            <div class="fragment" style="float: right; padding-right: 100px">
                <img src="img/parallel-knive.jpg" height="400">
                <p><small><em>parallel</em>, <br>fast but same operation for all data
                        </small>
                </p>
            </div>
        </section>


<section>
    <h3>Architectures of Convolutional Neural Networks: VGG</h3>
        <img src="img/sketch/vgg.png" height="350px">
        <p>
            <small>There are a number of specialized neural network layers</small>
        </p>
</section>

<!-- <section data-markdown>
    <textarea data-template>
### Classic VGG like Architecture
* we use a VGG like architecture
* based on https://arxiv.org/abs/1409.1556
* basic idea: sequential, deep, small convolutional filters, use dropouts to reduce overfitting
* 16/19 layers are typical
* many architectures are based on that
</textarea>
</section> -->

<section data-markdown>
    <textarea data-template>
### Convolutional Blocks: Cascading many Convolutional Layers having down sampling in between

![Applying filters](http://cs231n.github.io/assets/cnn/cnn.jpeg)

http://cs231n.github.io/convolutional-networks/#conv
</textarea>
</section>

<section data-markdown style="font-size: x-large">
    <textarea data-template>
### Example of a Convolution
![Dog](https://github.com/DJCordhose/speed-limit-signs/raw/master/img/conv/dog.png)
#### Many convolutional filters applied over all channels
![Dog after Convolutional Filters applied](https://github.com/DJCordhose/speed-limit-signs/raw/master/img/conv/dog-conv1.png)
http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html
</textarea>
</section>


<section data-markdown>
    <textarea data-template>
### Downlsampling Layer: Reduces data sizes and risk of overfitting
![Pooling](http://cs231n.github.io/assets/cnn/pool.jpeg)
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Max Pooling
![Max Pooling](http://cs231n.github.io/assets/cnn/maxpool.jpeg)
http://cs231n.github.io/convolutional-networks/#pool
</textarea>
</section>


<section data-markdown>
    <textarea data-template>
### Standard CNN Architecture

![Performance of CNN Architectures](https://cdn-images-1.medium.com/max/1600/1*kBpEOy4fzLiFxRLjpxAX6A.png)

<small>
https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
    ### Typical Architecture of a CNN 
![VGG architecture](img/sketch/vgg.png)

_The classifier more or less is what we used for our previous example_    
    </textarea>
    </section>

    <section>
            <h3>MNIST - Using a model <em>already trained</em></h3>
            <p>Exploring the different types layers together</p>
            <a href="https://transcranial.github.io/keras-js/#/mnist-cnn" target="_blank">
                <img src="img/browser/keras-browser.png" height="350px">
            </a>
            <p><small>
                <a href="https://transcranial.github.io/keras-js/#/mnist-cnn" target="_blank">https://transcranial.github.io/keras-js/#/mnist-cnn</a>
            </small></p>
        </section>

    <section>
            <h3>Keras layers</h3>

            <p><small>Convolution</small></p>
            <pre><code contenteditable data-trim class="fragment line-numbers javascript">
    model.add(Conv2D(filters=32, padding='same', activation='relu'))
                </code></pre>

                <p><small>Max Pooling</small></p>
                <pre><code contenteditable data-trim class="fragment line-numbers javascript">
model.add(MaxPooling2D())
                </code></pre>
                                    
                <p><small>Flatten 2d to make it accessible to Dense layers</small></p>
            <pre><code contenteditable data-trim class="fragment line-numbers javascript">
model.add(Flatten())
            </code></pre>
        <p>
            <small>
                    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers">
                        https://www.tensorflow.org/api_docs/python/tf/keras/layers
                    </a>
            </small>
        </p>
    </section>
    
    
        
        <section>
            <h3>More complex architecture: Google Inception V3</h3>
            <img src="img/inception_v3_architecture.png" height="400px">
            <p>
                <small>
                    Paper: <a href="https://arxiv.org/abs/1409.4842" target="_blank">Going Deeper with Convolutions</a>
                    <br>
                    <a href="https://stackoverflow.com/questions/39352108/does-the-inception-model-have-two-softmax-outputs" target="_blank">
                    Why two classifiers?</a>
                </small>
            </p>
        </section>

                </section>
        <section data-markdown>
                <textarea data-template>
### Fashion MNINST example

28x28 grayscale images of fashion Items

<img src="img/fashion-mnist-sprite.png" height="300px">

<small>
Tutorial: https://medium.com/tensorflow/hello-deep-learning-fashion-mnist-with-keras-50fcff8cd74a
<br><br>
<small>
https://colab.research.google.com/github/djcordhose/ai/blob/master/notebooks/tensorflow/nn-training.ipynb
</small>        
</small>
        </textarea>
        </section>
        
        <section data-markdown>
                <textarea data-template>
### Exercise

_Can you improve the model for Fashion MNINST notebook?_

* other/more/less layers
* different sequence, less/more filters
* prevent overfitting even better 
</textarea>
</section>


    <section data-markdown>
        <textarea data-template>
## RNNs
### Recurrent Neural Networks
        </textarea>
    </section>

        <section data-markdown>
        <textarea data-template>
### Challenge for traditional neural networks

How would you solve sequence to sequence translation?

Simple and theoretical example: addition digit by digit

```
216
+648
===
864
```


What is the challenge?
        </textarea>
    </section>
                        

    <section>
        <h3>Motivation</h3>
        <p>Traditional Networks have no memory of previous events</p>
        <p>Number to Number enconding needs to factor in carry</p>
    </section>

        
    <section data-markdown>
        <textarea data-template>
### Solution: RNNs - Networks with Loops
<img src='img/nlp/colah/RNN-rolled.png' height="450px">

<small>
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
</small>
        </textarea>
    </section>
        
    <section data-markdown>
        <textarea data-template>
### Unrolling the loop
<img src='img/nlp/colah/RNN-unrolled.png'>

<small>
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
</small>
        </textarea>
    </section>

    <section data-markdown>
        <textarea data-template>
### Simple RNN

<img src='img/nlp/fchollet_rnn.png'>

<script type="math/tex; mode=display">
output_t = \tanh(W input_t + U output_{t-1} + b)
</script>

<small>
<a href="https://livebook.manning.com/#!/book/deep-learning-with-python/chapter-6/129">
Deep Learning with Python, Chapter 6, François Chollet, Manning            
</a>

</small>

</textarea>
</section>

    <section data-markdown>
            <textarea data-template>
### Generating musical sequences        

Training a latent space and generating a new sequences

<img src='img/nsynth-ae.png'>

<small>
https://magenta.tensorflow.org/music-vae
</small>
</textarea>
</section>
    
    <section data-markdown>
            <textarea data-template>
### Also perfect for natural language Sequence to Sequence translations

<img src='img/nlp/encdec.jpg'>

<small>
https://www.tensorflow.org/tutorials/seq2seq
</small>
</textarea>
</section>
    <section data-markdown>
            <textarea data-template>
### Encoding addition as seq2seq

<img src='img/nlp/rnn-adder-input.png' height="500px">
                </textarea>
                </section>
    
    <section data-markdown>
            <textarea data-template>
### Each time step generates a digit of the result

<img src='img/nlp/rnn-adder-output.png' height="500px">
                </textarea>
                </section>

    <section data-markdown>
    <textarea data-template>
### Seeing the network at work in a notebook


```
Input: "216+648"
Output: "864"
```

Padding is handled by using a repeated sentinel character (space)

<small>
Notebook: https://colab.research.google.com/github/djcordhose/ai/blob/master/notebooks/nlp/rnn-add-example.ipynb
<br>
https://machinelearningmastery.com/learn-add-numbers-seq2seq-recurrent-neural-networks/
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Exercise

_Run the notebook and make experiments_

* Reverse the sequence of time stamps (there is a flag for it) - why might this be beneficial?
* Change the encoding of the input, maybe just a single character per time stamp

Can you improve on the results?
<small>
https://colab.research.google.com/github/djcordhose/ai/blob/master/notebooks/nlp/rnn-add-example.ipynb
</small>        
    </textarea>
</section>


<section data-markdown>
        <textarea data-template>
### Example Application: Using sequences of events
<img src='img/magenta-rnn-duck.png' height="400px">

<small>
https://twitter.com/random_forests/status/987394050914385927
<br>
https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html
</small>
        </textarea>
    </section>

    <section data-markdown>
            <textarea data-template>
### Main issues with RNNs

_Vanishing or exploding gradient problem:_
* Each step in training applies the same weights to the output, also in back-propagation  
* The further we move backwards, the bigger (explodes) or smaller (vanishes) the gradient becomes

<small>
https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Intution of effect

_Effectively long term memory does not work:_

* RNNs experiences difficulty in memorising words from far away in the sequence
* Predictions based on most recent words only

<small>
    https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### GRU (Gated Recurrent Unit) / LSTMS (Long short-term memory)

_allow past information
to be reinjected at a later time, thus fighting the vanishing-gradient problem_

<small>
https://en.wikipedia.org/wiki/Long_short-term_memory
<br>            
<a href="https://www.manning.com/books/deep-learning-with-python">
    Deep Learning with Python, Chapter 6.2.2, François Chollet, Manning            
</a>            
https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be
<br>
<br>
https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm
<br>
<br>
https://arxiv.org/ftp/arxiv/papers/1701/1701.05923.pdf
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Motivation: What makes text and sequences so different?
<img src="img/tf/audio-image-text.png">

<small>
https://www.tensorflow.org/tutorials/word2vec
</small>
        </textarea>
</section>

<section data-markdown>
        <textarea data-template>
### One Hot Encoding
<img src="img/nlp/acolyer/word2vec-one-hot.png">
<small>
https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
</small>
        </textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Limitations of One Hot Encoding / Bag of Words

* Positions and context of words get lost
* No notion of semantics to words

    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
<img src="img/nlp/word_embeddings.png" height="550px">

<small>
<a href="https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb">
Deep Learning with Python
</a>
</small>
    </textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Visualizing Embeddings

<a href='https://projector.tensorflow.org'>
<img src="img/nlp/embedding-projector.png" height="500px">
</a>

<small>
https://projector.tensorflow.org
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Exercise: Get an intuition for Word Embeddings

* Switch to T-SNE projections
* Zoom into a cluster
* Have a look at the words in the cluster
* Are they semantically related?

<small>
https://projector.tensorflow.org
</small>

</textarea>
</section>


<section data-markdown>
        <textarea data-template>
## Creating our very own sentiment analysis
### Using embeddings to train recurrent networks

Notebooks:
<small>
* https://colab.research.google.com/github/djcordhose/ai/blob/master/notebooks/nlp/2-rnn.ipynb
* https://colab.research.google.com/github/djcordhose/ai/blob/master/notebooks/nlp/2-lstm.ipynb
* https://colab.research.google.com/github/djcordhose/ai/blob/master/notebooks/nlp/3-gru-dropout.ipynb (final version avoiding overfitting)

</small>
   
        </textarea>
        </section>

<section data-markdown>
        <textarea data-template>
### Exercise

* Tweak Regularisation
* Make the RNN bidirectional

Can you improve on the results?

https://keras.io/layers/wrappers/#bidirectional

    </textarea>
</section>


<section data-markdown>
        <textarea data-template>
### What's next?

<img src='img/colah-next.png'>

<small>
https://distill.pub/2016/augmented-rnns/
<br>
Attention is all you need: https://arxiv.org/pdf/1706.03762.pdf
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Neural Machine Translation with Attention

_Using TensorFlow and Keras_

<img src='img/nmt-attention-twitter.png' height="400">

<small>
https://twitter.com/dennybritz/status/1011464747877838848/
</small>
</textarea>
</section>


    </div>

</div>

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>
<script src="lib/jquery-2.2.4.js"></script>

<script>
        if (window.location.hostname.indexOf('localhost') !== -1 && !printMode) {
        } else {
            // only applies to public version
            $('.todo').remove();
            $('.preparation').remove();
            $('.local').remove();
        }
        Reveal.addEventListener( 'ready', function( event ) {
            // do we want this???
            $('li').addClass('fragment')

            if (window.location.hostname.indexOf('localhost') !== -1) {
                // only applies to presentation version
                Reveal.configure({ controls: false });
            } else {
                // only applies to public version
                $('.fragment').removeClass('fragment');
            }
            // applies to all versions
            $('code').addClass('line-numbers');

            // make all links open in new tab
            $('a').attr('target', '_blank')

        } );
        // $('section').attr('data-background-image', "backgrounds/light-metal.jpg");
        // $('section').attr('data-background-image', "backgrounds/pink.jpg");
        $('section').attr('data-background-image', "backgrounds/white.jpg");
        // $('section').attr('data-background-image', "backgrounds/murmel2.jpg");
        // $('section').attr('data-background', "img/manning/background/m0.jpg");
        // $('section').attr('data-background', "img/manning/background/m1.jpg");
        // $('section:not([data-background])').attr('data-background', "img/manning/background/m1.jpg");
        // $('section').attr('data-background-size', "1620px");

    //    $('section').attr('data-background-image', "backgrounds/code.jpg");
    </script>
    
<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: false,
        history: true,
        center: true,
        width: 1100,

        transition: 'fade', // none/fade/slide/convex/concave/zoom

        math: {
            mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

        // Optional reveal.js plugins
        dependencies: [
            {
                src: 'reveal.js/lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: 'reveal.js/plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: 'reveal.js/plugin/zoom-js/zoom.js', async: true},
            {src: 'reveal.js/plugin/notes/notes.js', async: true},
            // https://github.com/mikemiles86/reveal-line-numbers
            {src: 'lib/js/line-numbers.js'},
            { src: 'reveal.js/plugin/math/math.js', async: true }
        ]
    });

</script>

</body>
</html>
