<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Unsupervised TensorFlow</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

          <link rel="stylesheet" href="reveal.js/css/reveal.css">
          <!--<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">-->
          <link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
      
          <!-- Code syntax highlighting -->
          <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
          <style>
              /*pre code {*/
                  /*display: block;*/
                  /*padding: 0.5em;*/
                  /*background: #FFFFFF !important;*/
                  /*color: #000000 !important;*/
              /*}*/
      
              .right-img {
                  margin-left: 10px !important;
                  float: right;
                  height: 500px;
              }
              .todo:before {
                  content: 'TODO: ';
              }
              .todo {
                  color: red !important;
              }
              code span.line-number {
                  color: lightcoral;
              }
              .reveal pre code {
                  max-height: 1000px !important;
              }
      
              img {
                  border: 0 !important;
                  box-shadow:0 0 0 0 !important;
              }
      
              .reveal {
                  -ms-touch-action: auto !important;
                  touch-action: auto !important;
                      }
      
                      .reveal h2,
                      .reveal h3,
                      .reveal h4 {
                        letter-spacing: 2px;
                          font-family: 'Calibri', sans-serif;
                          /* font-family: 'Times New Roman', Times, serif; */
                          font-weight: bold;
                          color: black;
                          font-style: italic;
                          letter-spacing: -2px;
                          text-transform: none !important;
                      }
      
                      .reveal em {
                          font-weight: bold;
                      }
      
                      .reveal .step-subtitle h1 {
                          letter-spacing: 1px;
                      }
                      .reveal .step-subtitle h2,
                      .reveal .step-subtitle h3 {
                          text-transform: none;
                          font-style: italic;
                          font-weight: normal;
                          /* font-weight: 400; */
                          /* font-family: 'Amiri', serif; */
                          font-family: 'Lobster', serif;
                          letter-spacing: 1px;
                          color: #2aa198;
                          text-decoration: underline;
                      }
      
                      .reveal .front-page h1,
                      .reveal .front-page h2 {
                          font-family: "League Gothic";
                          font-style: normal;
                          text-transform: uppercase !important;
                          letter-spacing: 1px;
                      }
      
                      .reveal .front-page h1 {
                          font-size: 2.5em !important;
                      }
      
                      .reveal .highlight {
                          background-color: #D3337B;
                          color: white;
                      }
      
              .reveal section img {
                background: none;
              }
      
                      .reveal img.with-border {
                          border: 1px solid #586e75 !important;
                          box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
                      }
      
                      .reveal li {
                          margin-bottom: 8px;
                      }
      
                      /* For li's that use FontAwesome icons as bullet-point */
                  .reveal ul.fa-ul li {
                      list-style-type: none;
                  }

            .reveal {
                color: black !important;
             }       

          </style>
      
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">
    <div class="slides">

<section data-markdown class="preparation">
        <textarea data-template>
### Preparation

* go through GAN visualization: https://poloclub.github.io/ganlab/
    </textarea>
</section>

<section>
        <h2>Unsupervised Learning with TensorFlow</h2>
        <h3>Optional: Advanced Keras and Stabilization for Production</h3>
<h4><a href="http://zeigermann.eu">Oliver Zeigermann</a> / <a href="http://twitter.com/djcordhose">@DJCordhose</a>
</h4>
<p><small><a href="https://djcordhose.github.io/ai/2019_tf_unsupervised.html">
https://djcordhose.github.io/ai/2019_tf_unsupervised.html
</a></small></p>
</section>

<section data-markdown>
        <textarea data-template>
### How to turn categories into numbers?

* neural networks can not deal with symbols
* only numerical values can be processed by neural networks
* even words and texts can be seen as categories / symbols
* would it be possible to still make those numbers carry the semantics of the symbols? 

            </textarea>
            </section>

<section data-markdown>
    <textarea data-template>
### Motivating Exercise: find a numerical representation for airports 

_team up and work on paper_

* choose a few airports of your choice and bring them into numbers
* why did you choose this representation?
* do you think it is a good one?
</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Label Encoding    

Normalize symbols such that they contain only values between 0 and number_of_symbols_in_vocab-1.

<pre><code>text = ["paris", "paris", "tokyo", "amsterdam"]
paris = 0
tokyo = 1
amsterdam = 2
encoded_text = [0, 0, 1, 2]</code></pre>

<small>
http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html
</small>
</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Issues with turning symbols into integers

* numbers close to each other suggest a relation, but there might actually be none
* what would 7.5 mean?
* and what -8457878574 mean?
</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Bag of Words - One/Multiple Hot Encoding
<img src="img/nlp/acolyer/word2vec-one-hot.png">
<small>
https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Issues with One-Hot-Encoding

* high dimensionalty 
* sparse representation
* neighborhood does not mean anything

            </textarea>
            </section>

            <section data-markdown>
                    <textarea data-template>
### Enter: Embeddings

* Embedding: Transform a high dim. vector space to a lower one
* word/symbol Embedding: Transform sparse one hot encodings into a dense lower dim. encoding 

<small>https://en.wikipedia.org/wiki/Word_embedding</small>
                        </textarea>
                    </section>
                
                    <section data-markdown>
<textarea data-template>
<img src="img/nlp/word_embeddings.png" height="550px">

<small>
<a href="https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb">
Deep Learning with Python
</a>
</small>
</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Assumption: symbols/words in similar contexts have similar meaning

* Why not have a few semantic dimensions and embed symbols/words into them?
* You define what is context and thus gives meaning
* For random texts this might be just the words that surround others
        </textarea>
    </section>

<section data-markdown>
    <textarea data-template>
## Types of Learning
<img src='img/types-of-ml.jpg'>
<small>
https://www.facebook.com/nipsfoundation/posts/795861577420073/
<br>
https://ranzato.github.io/publications/tutorial_deep_unsup_learning_part1_NeurIPS2018.pdf
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
#### The Brain Maps Out Ideas and Memories Like Spaces

<img src='img/brain-abstract-knowledge.jpg' height="450">

_might it make sense to represent abstract knowledge in numbers?_

<small>
https://twitter.com/PhilosophyMttrs/status/1085242776688775169
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Idea of Neural Embedding

* You define a network using an embedding layer as input
* What you want to encode goes into network as a number
* You represent its semantics as the output
* You train the model
* You only keep the embedding layer that contains the abstraction

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Sketch of such a network

<img src='img/sketch/sketch_idea_embedding.png' height="550px">

</textarea>
</section>

<section>
    <h3>Network to train embedding</h3>

    <p><small>Input</small></p>
    <pre><code contenteditable data-trim class="fragment line-numbers python">
number_of_airports = len(airports)
embedding_dim = 1
sequence_length = 1

model.add(Embedding(input_dim=number_of_airports, 
                    output_dim=embedding_dim, input_length=sequence_length))
</code></pre>

    <p><small>Output</small></p>
    <pre><code contenteditable data-trim class="fragment line-numbers python">
# embedding will be 2d, but Dense can only handle flat input
model.add(Flatten())

# random 
model.add(Dense(units=50, activation='relu'))

model.add(Dense(units=dictionary_size, name='output', activation='softmax'))
</code></pre>

</p>
</section>

<section data-markdown>
    <textarea data-template>
### Embedding Airports in the notebook

_also shows encoding of input and output_

<img src='img/embedding_airport.png'>

<small style="font-size: large">
https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/2019_tf/embeddings.ipynb
</small>
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Arithmetic on Embeddings

_has been exaggerated recently, but still fun_

<img src='img/embedding_arithmetic.jpg' height="400px">

<small>
http://bryanlohjy.gitlab.io/spacesheet/word2vec.html
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Suprising Application of Embeddings

<img src='img/embedding-spell-checker.png' height="500px">

<small>
https://twitter.com/jeremyphoward/status/997264148655259648    
</small>
        </textarea>
        </section>

<section data-markdown>
        <textarea data-template>
### Word Embeddings using word2vec

_main assumption: words appearing in similar contexts have similar meaning_

<a href='https://projector.tensorflow.org'>
<img src="img/nlp/embedding-projector.png" height="350px">
</a>

<small>
https://projector.tensorflow.org
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Hands-On: Find an application for an embedding 

* Run the introductory notebook
* can you come up with an interpretation for the semantics of the two dimensions of the embedding?
* then either 
  1. change the representation for airports and try to achieve good results
  1. come up with an application of your own (this is hard, make sure it still fits into the structure of the notebook) 

<small style="font-size: large">
https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/2019_tf/embeddings.ipynb
</small>
</textarea>
</section>


<section data-markdown>
    <textarea data-template>
### There is more to Unsupervised Deep Learning

* Autoencoders
  * VAEs
* Generative Adversarial Network (GAN) 

</textarea>
</section>

<!-- <section data-markdown style="font-size: x-large">
    <textarea data-template>
### Unsupervised Deep Learning 

Workshop at NIPS 2018
* https://www.facebook.com/nipsfoundation/posts/795861577420073/
* Slides: https://ranzato.github.io
  * https://ranzato.github.io/publications/tutorial_deep_unsup_learning_part1_NeurIPS2018.pdf
  * https://ranzato.github.io/publications/tutorial_deep_unsup_learning_part2_NeurIPS2018.pdf

</textarea>
</section> -->

<section data-markdown>
    <textarea data-template>
### Autoencoders

* reproduce an input while going through a bottleneck
* latent representation is what you are interested in

<img src='img/autoencoder_schema.jpg'>

<small>
https://blog.keras.io/building-autoencoders-in-keras.html
</small>
        
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Why Autoencoders

* compression
* data denoising
* dimensionality reduction / clustering (for data visualization)
* building an abstract representation for further use
* works on all kinds of data, e.g. image, audio, and tabular

<small>
https://blog.keras.io/building-autoencoders-in-keras.html
</small>
        
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Variational Auto Encoders (VAE)

* VAE is a generative model
* latent space learns a probability distribution modelling your data
* actually learning mean and standard deviation of distribution
* sampling from it can generate new data

<small>
https://blog.keras.io/building-autoencoders-in-keras.html
<br>
https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py
</small>
</textarea>
    </section>

    <section data-markdown>
        <textarea data-template>
### VAE illustrated

<img src='img/vae.png' height="500px">

<small>
https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf
</small>
</textarea>
    </section>

<!-- <section data-markdown class="todo">
    <textarea data-template>
### VAE

* Intro: https://youtu.be/9zKuYvjFFS8
* https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/generative_examples/cvae.ipynb
* Sean's Notebook: https://colab.research.google.com/drive/1f73wONMp8U2LvAmN0MNGyflqGFog0g2S
* VAE: 
  * http://tiao.io/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/

</textarea>
</section> -->

<section data-markdown>
    <textarea data-template>
## GANs

</textarea>
</section>

<section>
<h3>Generating Celebreties</h3>
<p>Trained for two weeks on a single high-end GPU on CelebA-HQ data set (images of celebreties)</p>
<div class="fragment" style="float: left">
    <img src="img/unsupervised/gan-model-male2.png" height="220">
</div>
<div class="fragment" style="float: left; padding-left: 25px">
        <img src="img/unsupervised/gan-model-female2.png" height="220">
</div>
<div class="fragment" style="float: left; padding-left: 25px">
        <img src="img/unsupervised/gan-model-female1.png" height="220">
</div>
<div class="fragment" style="float: left; padding-left: 25px">
        <img src="img/unsupervised/gan-model-male.png" height="220">
</div>
<p style="clear: both">
<small>
<a href="https://alantian.net/ganshowcase/" target="_blank">https://alantian.net/ganshowcase/</a>
<br>
<a href="https://github.com/alantian/ganshowcase" target="_blank">https://github.com/alantian/ganshowcase</a>
<br>
<a href="https://twitter.com/alanyttian/status/988242167998148608" target="_blank">https://twitter.com/alanyttian/status/988242167998148608</a>
</small>
                   
</p>
</section>                                

<section data-markdown>
    <textarea data-template>
### Understanding GANs

<a href='https://poloclub.github.io/ganlab/'>
<img src='img/tfjs/gan-lab.png'>
</a>
        
<small>
https://twitter.com/minsukkahng/status/1037016214575505409
https://poloclub.github.io/ganlab/
https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf
</small>
</textarea>
</section>

<!-- <section data-markdown class="todo">
    <textarea data-template>
### GANS    

New blog post: "GANs and Divergence Minimization", which covers the perspective of GANs as minimizing an "adversarial divergence" and draws parallels to maximum likelihood training. Also provides some motivation for better evaluation of GANs. https://t.co/XibWmoWEw8 https://t.co/tik01df1aS
(https://twitter.com/colinraffel/status/1076179243678093312?s=03)
</textarea>
</section> -->


<!-- <section data-markdown>
    <textarea data-template>
### Introduction to Autoencoders

_reproducing the MNIST data set_

<img src='img/mninst_ae.jpg'>

<small style="font-size: large">
https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/2019_tf/autoencoders_intro.ipynb
</small>
        
</textarea>
</section> -->

<section data-markdown>
    <textarea data-template>
## From here on advanced and optional material

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Autoencoding tabular data

<img src='img/tabular-data-pairplot.png' height="550px">

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Interative Exercise: Sketch a network for an Autoencoder on Tabular Data

<!-- <img src='img/sketch/sketch_idea_embedding.png' height="550px"> -->

</textarea>
</section>


<section>
    <h3>Autoencoder Network in Keras</h3>

    <p><small>Encoder</small></p>
    <pre><code contenteditable data-trim class="fragment line-numbers python">
# defines size (dimension) of latent space
encoding_dim = ???

# defines range of latent space
encoding_activation = ???

# https://keras.io/getting-started/functional-api-guide/
input = Input(shape=(4,))
encoded = Dense(units=encoding_dim, activation=encoding_activation,
                name="decoder")(input)
</code></pre>

    <p><small>Output</small></p>
    <pre><code contenteditable data-trim class="fragment line-numbers python">
# decoder can be extremely simple
decoded = Dense(units=4, activation='linear', name="decoder")(encoded)

autoencoder = Model(inputs=input, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='mse')
</code></pre>

</p>
</section>

<section data-markdown>
    <textarea data-template>
### Notebook: Autoencoding tabular data

<img src='img/insurance_ae.png'>

<small style="font-size: large">
https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/2019_tf/autoencoders_tabular.ipynb
</small>
        
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Issues

1. Should we not try to encode groups more reasonable?
   * we have used one-hot-encoding before
1. How to we stabilize model when new data comes in (apart from initialization seed)?
   * people do not want to see drastic changes in visualizations when just a few data points change

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### What has the biggest impact on our trained model? 

<img src='img/sean-data.png'>

<small>
https://twitter.com/SeanPedersen96/status/1063272488489099264
<br>
https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Challenge: How to deal with two different types of input/outputs?

* What loss functions?
* How to combine the loss? 

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Interative Exercise: Sketch a network for an advanced autoencoders

_let's restrict ourselves to the first issue for now: one-hot-encoding_

</textarea>
</section>


<section>
    <h3>One-Hot-Encoding Groups</h3>

    <pre><code contenteditable data-trim class="fragment python">
# first multi in
main_in = Input(shape=(3,), name='main_input')
group_in = Input(shape=(3,), name='group_input')
</code></pre>

    <pre><code contenteditable data-trim class="fragment python">
# slightly simplified latent encoding
merged = concatenate([main_in, group_in])
encoding_dim = 2
encoded = Dense(units=encoding_dim, activation='relu', name="encoder")(merged)
</code></pre>

    <pre><code contenteditable data-trim class="fragment python">
# then multi out
main_out = Dense(units=3, activation='linear', name="main_output")(encoded)
group_out = Dense(units=3, activation='softmax', name="group_output")(encoded)
</code></pre>
    <pre><code contenteditable data-trim class="fragment python">
# separate loss for each output, scaled to same order of magnitude
autoencoder = Model(inputs=[main_in, group_in], outputs=[main_out, group_out])
autoencoder.compile(optimizer='adam',
                    loss={'main_output': 'mae', 
                          'group_output': 'categorical_crossentropy'},
                    loss_weights={'main_output': 1., 'group_output': 50.})
</code></pre>

</p>
</section>

<section data-markdown>
    <textarea data-template>
### Advanced Autoencoders 

only look at first part for one-hot-encoding, stop before stabilization part 

<img src='img/ae-groups.png'>

<small style="font-size: large">
https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/2019_tf/autoencoders_stabilize.ipynb
</small>
        
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Exercise: Optimize training

_try to generate a more telling visualizations_

* make sure you understand the architecture
* tune loss ratios
* sizes of input encodings

<small style="font-size: large">
https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/2019_tf/autoencoders_stabilize.ipynb
</small>
        
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Brainstorming: How to achieve stabilization?

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Advanced Autoencoders, stabilized for production 

<small style="font-size: large">
        https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/2019_tf/autoencoders_stabilize.ipynb#scrollTo=we2ONI5pBR8R
</small>
        
</textarea>
</section>


    </div>

</div>

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>
<script src="lib/jquery-2.2.4.js"></script>

<script>
        $('.slido').remove();
        if (window.location.hostname.indexOf('localhost') !== -1) {
            // only applies to local version
            $('.remote').remove();
        } else {
            // only applies to public version
            $('.todo').remove();
            $('.preparation').remove();
            $('.local').remove();
        }
        Reveal.addEventListener( 'ready', function( event ) {
                // do we want this???
            $('li').addClass('fragment')

            if (window.location.hostname.indexOf('localhost') !== -1) {
                // only applies to presentation version
                Reveal.configure({ controls: false });
            } else {
                // only applies to public version
                $('.fragment').removeClass('fragment');
            }
            // applies to all versions
            $('code').addClass('line-numbers');

            // make all links open in new tab
            $('a').attr('target', '_blank')

        } );
        // $('section').attr('data-background-image', "backgrounds/light-metal.jpg");
        // $('section').attr('data-background-image', "backgrounds/pink.jpg");
        // $('section').attr('data-background-image', "backgrounds/white.jpg");
        $('section').attr('data-background-image', "backgrounds/sky.jpg");

    //    $('section').attr('data-background-image', "backgrounds/code.jpg");
    </script>
    
<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        width: 1100,


        transition: 'fade', // none/fade/slide/convex/concave/zoom

        math: {
            mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

        // Optional reveal.js plugins
        dependencies: [
            {
                src: 'reveal.js/lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: 'reveal.js/plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: 'reveal.js/plugin/zoom-js/zoom.js', async: true},
            {src: 'reveal.js/plugin/notes/notes.js', async: true},
            // https://github.com/mikemiles86/reveal-line-numbers
            {src: 'lib/js/line-numbers.js'},
            { src: 'reveal.js/plugin/math/math.js', async: true }
        ]
    });

</script>

</body>
</html>
