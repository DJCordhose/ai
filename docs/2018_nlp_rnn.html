<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>RNNs</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

          <link rel="stylesheet" href="reveal.js/css/reveal.css">
          <!--<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">-->
          <link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
      
          <!-- Code syntax highlighting -->
          <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
          <style>
              /*pre code {*/
                  /*display: block;*/
                  /*padding: 0.5em;*/
                  /*background: #FFFFFF !important;*/
                  /*color: #000000 !important;*/
              /*}*/
      
              .right-img {
                  margin-left: 10px !important;
                  float: right;
                  height: 500px;
              }
              .todo:before {
                  content: 'TODO: ';
              }
              .todo {
                  color: red !important;
              }
              code span.line-number {
                  color: lightcoral;
              }
              .reveal pre code {
                  max-height: 1000px !important;
              }
      
              img {
                  border: 0 !important;
                  box-shadow:0 0 0 0 !important;
              }
      
              .reveal {
                  -ms-touch-action: auto !important;
                  touch-action: auto !important;
                      }
      
                      .reveal h2,
                      .reveal h3,
                      .reveal h4 {
                          letter-spacing: 2px;
                          font-family: 'Amiri', serif;
                          /* font-family: 'Times New Roman', Times, serif; */
                          font-weight: bold;
                          font-style: italic;
                          letter-spacing: -2px;
                          text-transform: none !important;
                      }
      
                      .reveal em {
                          font-weight: bold;
                      }
      
                      .reveal .step-subtitle h1 {
                          letter-spacing: 1px;
                      }
                      .reveal .step-subtitle h2,
                      .reveal .step-subtitle h3 {
                          text-transform: none;
                          font-style: italic;
                          font-weight: normal;
                          /* font-weight: 400; */
                          /* font-family: 'Amiri', serif; */
                          font-family: 'Lobster', serif;
                          letter-spacing: 1px;
                          color: #2aa198;
                          text-decoration: underline;
                      }
      
                      .reveal .front-page h1,
                      .reveal .front-page h2 {
                          font-family: "League Gothic";
                          font-style: normal;
                          text-transform: uppercase !important;
                          letter-spacing: 1px;
                      }
      
                      .reveal .front-page h1 {
                          font-size: 2.5em !important;
                      }
      
                      .reveal .highlight {
                          background-color: #D3337B;
                          color: white;
                      }
      
              .reveal section img {
                background: none;
              }
      
                      .reveal img.with-border {
                          border: 1px solid #586e75 !important;
                          box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
                      }
      
                      .reveal li {
                          margin-bottom: 8px;
                      }
      
                      /* For li's that use FontAwesome icons as bullet-point */
                  .reveal ul.fa-ul li {
                      list-style-type: none;
                  }
          </style>
      
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">
    <div class="slides">

            <section data-markdown class="preparation" style="font-size: xx-large">
                    <textarea data-template>
### Preparation

Insgesamt 90 Minuten für den ganzen Satz. Die Folien gehen in 2
Teilen, die zeitlich getrennt sein können:
1. Teil RNNs/LSTMs: Sehr kurze Zusammenfassung / Wiederholung - Vortrag von dem
Vortrag des Studenten mit Notebook Addierer mit RNNs
1. Teil GRUs im Detail mit Notebook / Anwendung Sentiment Analysis

Beide Notebooks noch mal ansehen:
1. rnn-add-example
1. gru-sandbox
                        </textarea>
                        </section>
        

        <section>
            <h2>Recurrent Neural Networks</h2>
            <h4><a href="http://zeigermann.eu">Oliver Zeigermann</a> / <a href="http://twitter.com/djcordhose">@DJCordhose</a>
            </h4>
            <small>
                    <a href="https://djcordhose.github.io/ai/2018_nlp_rnn.html">
                        https://djcordhose.github.io/ai/2018_nlp_rnn.html</a>
                        </small>

        </section>

        <!-- <section class="todo">
            <pre>
            </pre>
        </section> -->

        <section>
                <h2>Structure</h2>
                <ol>
                    <li>Machine Learning architectures applicable to text
                            <ol>
                                    <li>RNNs</li>
                                    <li>LSTMs</li>
                                    <li>GRUs</li>
                                    <li>Applications</li>
                                </ol>
                    </li>
                </ol>
            </section>
    
    
    <section>
            <h3>Text is special</h3>
            <img src='img/applications/decisions/data.png'>
        </section>


        <section>
            <h2>Simple RNNs</h2>
            <h3>Recurrent Neural Networks</h3>
        </section>


        <section>
            <h3>Motivation</h3>
            <p>Traditional Networks have no memory of previous events</p>
        </section>

            
        <section data-markdown>
            <textarea data-template>
### Solution: RNNs - Networks with Loops
<img src='img/nlp/colah/RNN-rolled.png' height="450px">

<small>
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
</small>
            </textarea>
        </section>
            
        <section data-markdown>
            <textarea data-template>
### Unrolling the loop
<img src='img/nlp/colah/RNN-unrolled.png'>

<small>
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
</small>
            </textarea>
        </section>

        <section data-markdown>
            <textarea data-template>
### Simple RNN

<img src='img/nlp/fchollet_rnn.png'>

<script type="math/tex; mode=display">
output_t = \tanh(W input_t + U output_{t-1} + b)
</script>
    
<small>
<a href="https://livebook.manning.com/#!/book/deep-learning-with-python/chapter-6/129">
    Deep Learning with Python, Chapter 6, François Chollet, Manning            
</a>

</small>

</textarea>
</section>

<section>
    <h3>Repetition Activation Functions</h3>
    <div style="float: left">
            <img src="img/sigmoid-activation.png" height="300px">
            <p>
                <small>Sigmoid, floating from 0 to 1</small>
            </p>
        </div>
        <div style="float: right">
                <img src="img/tanh-activation.png" height="300px">
                <p>
                <small>Tangens Hyperbolicus, floating from -1 to 1</small>
            </p>
        </div>
    <small>
        <a href="https://notebooks.azure.com/djcordhose/libraries/buch/html/kap7-iris.ipynb">
            https://notebooks.azure.com/djcordhose/libraries/buch/html/kap7-iris.ipynb
    </a>
    </small>    
</section>

<section>
            <h2>Perfect for sequences, lists, sentences</h2>
        </section>
        
        <section data-markdown>
                <textarea data-template>
### Example: Using sequences of events
<img src='img/magenta-rnn-duck.png' height="450px">

<small>
https://twitter.com/random_forests/status/987394050914385927
</small>
                </textarea>
            </section>

        <section data-markdown>
        <textarea data-template>
### Example application for RNNs

Add two numbers encoded in a string, e.g.

```
Input: "535+61"
Output: "596"
```
Padding is handled by using a repeated sentinel character (space)

<small>
Notebook: rnn-add-example
</small>
    </textarea>
    </section>


        <section data-markdown>
                <textarea data-template>
### Main issues with RNNs

_Vanishing or exploding gradient problem:_
* Each step in training applies the same weights to the output, also in back-propagation  
* The further we move backwards, the bigger (explodes) or smaller (vanishes) our error signal becomes

<small>
https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7
</small>
</textarea>
    </section>

    <section data-markdown>
            <textarea data-template>
### Intution of effect

_Effectively long term memory does not work:_

* RNNs experiences difficulty in memorising words from far away in the sequence
* Predictions based on most recent words only

<small>
        https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7
    </small>
    </textarea>
</section>
    <section data-markdown>
                <textarea data-template>
## LSTMs
</textarea>
    </section>

            <section data-markdown>
                    <textarea data-template>
### Starting from a Simple RNN

<img src='img/nlp/fchollet_rnn.png'>

<small>
<a href="https://livebook.manning.com/#!/book/deep-learning-with-python/chapter-6/129">
            Deep Learning with Python, Chapter 6, François Chollet, Manning            
    </a>

</small>

        </textarea>
        </section>

        <section data-markdown>
                <textarea data-template>
### Adding Carry
Modulate the next output and the next state via a carry across timesteps
<img src='img/nlp/fchollet_rnn_carry.png'>

<small>
<a href="https://livebook.manning.com/#!/book/deep-learning-with-python/chapter-6/161">
        Deep Learning with Python, Chapter 6, François Chollet, Manning            
</a>
</small>
    </textarea>
    </section>

<section data-markdown>
        <textarea data-template>
### Complete LSTM
New carry is computed from input, output and previous carry

<img src='img/nlp/fchollet_lstm.png'>

<small>
<a href="https://livebook.manning.com/#!/book/deep-learning-with-python/chapter-6/167">
Deep Learning with Python, Chapter 6, François Chollet, Manning            
</a>
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### LSTM Intution

gates manage what memory to take over

1. Forget Gate: forget irrelevant information in the carry dataflow (one weights matrix)
1. Keep Gate: provide information about the present, updating the carry track with new information (two weights matrices)

<small>
https://arxiv.org/ftp/arxiv/papers/1701/1701.05923.pdf
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### ... but don't overdo it

In summary: you don’t need to understand anything
about the specific architecture of an LSTM cell; as a human, it shouldn’t be your job to
understand it. Just keep in mind what the LSTM cell is meant to do: allow past information
to be reinjected at a later time, thus fighting the vanishing-gradient problem.

<small>
<a href="https://www.manning.com/books/deep-learning-with-python">
Deep Learning with Python, Chapter 6.2.2, François Chollet, Manning            
</a>
</small>
</textarea>
</section>
        <section data-markdown>
                <textarea data-template>
### GRU (Gated Recurrent Unit)

As powerful as LSTMs, but simpler

* Invented in 2014
* variation of LSTMs

<small>
https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be
<br>
https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm
<br>
https://arxiv.org/ftp/arxiv/papers/1701/1701.05923.pdf
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
<img src='img/nlp/gru-overview.png' height="500px">
<img src='img/nlp/gru-legend.png'>

</textarea>
</section>
        

<section data-markdown>
        <textarea data-template>
### Feeding input over time into a single unit            

<img src='img/nlp/gru-timestamps.png'>

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Gates

* Decide which information is passed to output
* Can be trained to keep information from long ago
* Can decide to copy all the information from the past eliminating the risk of vanishing gradient problem
* Update Gate
* Reset Gate

<small>
https://arxiv.org/pdf/1406.1078v3.pdf
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Intution

* _reset gates_ can be trained to _wash out previous memories_ if they turn out to be irrelevant
* _update gates_ can be trained to much more _rely on past memories_ if they are more relevant to the output

    </textarea>
</section>
        

<section data-markdown>
        <textarea data-template>
### Update Gate

<img src='img/nlp/gru-update-gate.png' height="450px">

<small>
Helps to determine how much of the past information (from previous time steps) needs to be passed along to the future.
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Update Gate

<br>        
<script type="math/tex; mode=display">
z_t = \sigma(W(z)x_t + U(z)h_{t-1})
</script>

<small>
* When x(t) is plugged into the network unit, it is multiplied by its own weight W(z). 
* The same goes for h(t-1) which holds the information for the previous t-1 units and is multiplied by its own weight U(z). 
* Both results are added together and a sigmoid activation function is applied to squash the result between 0 and 1. 

</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Reset Gate


<img src='img/nlp/gru-reset-gate.png' height="450px">

<small>
Used to decide how much of the past information to forget
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Reset Gate

<br>        
<script type="math/tex; mode=display">
r_t = \sigma(W(r)x_t + U(r)h_{t-1})
</script>

<small>
* Same formula as for Update Gate 
* But other weights
* Different usage

</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Current memory content


<img src='img/nlp/gru-current-memory.png' height="450px">

<small>
Uses the reset gate to store relevant information from the past
</small>
</textarea>
</section>


<section data-markdown>
        <textarea data-template>
### Hadamard product

* Multiply two matrices of the same dimension
* Multiply entry by entry
* Results in a matrix of the same dimension

Example:

<img src='img/nlp/hadamard_example.svg'>

<small>
https://en.wikipedia.org/wiki/Hadamard_product_(matrices)
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Current memory content

<br>        
<script type="math/tex; mode=display">
h'_t = tanh(Wx_t + r_t \odot Uh_{t-1})
</script>

<small>
* Hadamard (element-wise) product will determine what to remove from the previous time step
* elements of r(t) will be between 0 and 1
* 0 will mean wash out past memories in this unit
* 1 means keep all past memories in this unit
* tanh makes overall output between -1 and 1

</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Final memory at current time step

<img src='img/nlp/gru-final-memory.png' height="450px">

<small>
Uses update gate to determines what to collect from the current memory content what from the past
</small>
</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Final memory at current time step

Final output is a weighted sum of past and current memory

<br>        
<script type="math/tex; mode=display">
h_t = z_t \odot h_{t-1} + (1 - z_t) \odot h'_{t} 
</script>

<small>
* h(t) is the final output of this unit 
* elements of update gate z(t) will be between 0 and 1
* h(t-1) as the input from the previous unit 
* 1 - z(t) will be the inverse
* h’(t) is the current memory content

</small>
</textarea>
</section>

    <section data-markdown style="font-size: xx-large">
            <textarea data-template>
## Applications
* _Speech Recognition_: https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a
* _Neural Machine Translation (seq2seq)_: https://www.tensorflow.org/tutorials/seq2seq
* _Sentiment Analysis_: https://medium.com/@alyafey22/sentiment-classification-from-keras-to-the-browser-7eda0d87cdc6
* _Text / Source Code Generators_: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
    </textarea>
    </section>

    <section data-markdown>
            <textarea data-template>
### Material from Stanford

<a href='http://web.stanford.edu/class/cs224n/syllabus.html'>
<img src='img/cs224n-syllabus.png'>
</a>

<small>
http://web.stanford.edu/class/cs224n/
</small>
</textarea>
</section>

    <section data-markdown>
            <textarea data-template>
### What's next?

<img src='img/colah-next.png'>

<small>
https://distill.pub/2016/augmented-rnns/
<br>
Attention is all you need: https://arxiv.org/pdf/1706.03762.pdf
</small>
</textarea>
</section>


    </div>

</div>

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>
<script src="lib/jquery-2.2.4.js"></script>

<script>
        if (window.location.hostname.indexOf('localhost') !== -1) {
        } else {
            // only applies to public version
            $('.todo').remove();
            $('.preparation').remove();
            $('.local').remove();
        }
        Reveal.addEventListener( 'ready', function( event ) {
            if (window.location.hostname.indexOf('localhost') !== -1) {
                // only applies to presentation version
                Reveal.configure({ controls: false });
            } else {
                // only applies to public version
                $('.fragment').removeClass('fragment');
            }
            // applies to all versions
            $('code').addClass('line-numbers');
        } );
        // $('section').attr('data-background-image', "backgrounds/light-metal.jpg");
        // $('section').attr('data-background-image', "backgrounds/pink.jpg");
        // $('section').attr('data-background-image', "backgrounds/white.jpg");
        $('section').attr('data-background-image', "backgrounds/leafes.jpg");
    //    $('section').attr('data-background-image', "backgrounds/code.jpg");
    </script>
    
<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'fade', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
            {
                src: 'reveal.js/lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: 'reveal.js/plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: 'reveal.js/plugin/zoom-js/zoom.js', async: true},
            {src: 'reveal.js/plugin/notes/notes.js', async: true},
            // https://github.com/mikemiles86/reveal-line-numbers
            {src: 'lib/js/line-numbers.js'},
            { src: 'reveal.js/plugin/math/math.js', async: true }
        ]
    });

</script>

</body>
</html>
